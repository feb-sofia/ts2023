[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "",
    "text": "General Information"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Schedule",
    "text": "Schedule\n\nTu 12:15-13:45 in room 302\nWe 12:15-13:45 in room 302\nWe 14:15-15:45 in room 308"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Grading",
    "text": "Grading\nThe course consists of two parts: lectures and exercise classes. Your final grade will be the average of the grades of these two parts. To obtain a grade for the exercise classes, you need to complete a homework project at the end of the semester—details to be announced."
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/feb-sofia/ts2023"
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer and accept the default settings.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nOptionally, you can also download and install git. In case of difficulties, these videos may help.\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\nhttps://github.com/feb-sofia/ts2023.git\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete.\n\n\ninstall.packages(c(\"tidyverse\", \"tidyverts\", \"xts\", \"quantmod\", \"urca\"))\n\n\n\n\nInstall packages\n\n\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r"
  },
  {
    "objectID": "index.html#recommended-reading",
    "href": "index.html#recommended-reading",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nKirchgässner, Wolters, and Hassler (2013)\nCowpertwait and Metcalfe (2009)\n\n\n\n\n\nCowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory Time Series with R. Use R! Dordrecht Heidelberg: Springer. https://doi.org/10.1007/978-0-387-88698-5.\n\n\nKirchgässner, Gebhard, Jürgen Wolters, and Uwe Hassler. 2013. Introduction to Modern Time Series Analysis. 2. ed. Springer Texts in Business and Economics. Berlin Heidelberg: Springer."
  },
  {
    "objectID": "01-Introduction.html#date-and-time-objects-in-r",
    "href": "01-Introduction.html#date-and-time-objects-in-r",
    "title": "1  Introduction",
    "section": "1.1 Date and Time objects in R",
    "text": "1.1 Date and Time objects in R\nInternally, dates are stored as the number of days since a (arbitrary) reference (origin). The default origin in R is the first of January 1970.\n\norigin &lt;- as.Date(\"1970-01-01\")\njan02.1970 &lt;- as.Date(\"1970-01-02\")\n\norigin\n\n[1] \"1970-01-01\"\n\njan02.1970\n\n[1] \"1970-01-02\"\n\nas.numeric(origin)\n\n[1] 0\n\nas.numeric(jan02.1970)\n\n[1] 1\n\norigin + 5\n\n[1] \"1970-01-06\"\n\norigin + 5:10\n\n[1] \"1970-01-06\" \"1970-01-07\" \"1970-01-08\" \"1970-01-09\" \"1970-01-10\"\n[6] \"1970-01-11\"\n\n\nTime is stored as the number of seconds (and fractions of a second) since the origin: 1970-01-01 00:00:00 UCT. POSIX stands for Portable Operating System Interface.\n\norigin_time &lt;- as.POSIXct(\"1970-01-01 00:00:00\", tz = \"UCT\")\nsome_other_time &lt;- as.POSIXct(\"1970-01-01 00:01:00\", tz = \"UCT\")\n\n# Print the value of origin_time\norigin_time\n\n[1] \"1970-01-01 UTC\"\n\n# Print the value of origin_time\nsome_other_time\n\n[1] \"1970-01-01 00:01:00 UTC\"\n\n# Print the numeric value of origin_time (seconds since the origin)\nas.numeric(origin_time)\n\n[1] 0\n\n# Print the numeric value of some_other_time (seconds since the origin)\nas.numeric(some_other_time)\n\n[1] 60\n\n\n\nclass(origin_time)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nThe package lubridate provides utility functions for working with date and time objects. You can look at this online guide here.\n\n# Sys.time returns the current time\nnow &lt;- Sys.time()\n\n# Print the value of now\nnow\n\n[1] \"2023-11-08 11:58:54 UTC\"\n\n# Day of the month\nday(now)\n\n[1] 8\n\n# Day of the week (depends on the start of week assumption)\nwday(now)\n\n[1] 4\n\nwday(now, week_start = 1)\n\n[1] 3\n\n# Day of the week as string (locale dependent)\nwday(now, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n# Day of the year\nyday(now)\n\n[1] 312\n\n# Week of the year\nweek(now)\n\n[1] 45\n\n# Month of the year\nmonth(now)\n\n[1] 11\n\n# Quarter of the year\nquarter(now)\n\n[1] 4\n\n\n\nExercise 1.1 (Date Objects in R)  \n\nWhat day of the week was the 10-th of June 2019?\nCreate a sequence of 34 consecutive dates starting on the 2-nd of January 2017. What was the year quarter of the last date of this sequence?\n\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "01-Introduction.html#time-series-classes",
    "href": "01-Introduction.html#time-series-classes",
    "title": "1  Introduction",
    "section": "1.2 Time Series Classes",
    "text": "1.2 Time Series Classes\nThere are several classes that are used for storing time series in R. We will mainly focus on tsibble, but some functions we will encounter later in the course return or require ts or xts objects.\n\nts\nxts (see the xts vignette)\ntsibble (see the introduction to tsibble)\n\n\n1.2.1 Creating ts Objects\n\nts(dt$x, start = 1960, frequency = 12)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1960 10.20000 10.40400 10.61208 10.82432 11.04081 11.26162 11.48686 11.71659\n1961 12.93607 13.19479 13.45868 13.72786 14.00241 14.28246 14.56811 14.85947\n          Sep      Oct      Nov      Dec\n1960 11.95093 12.18994 12.43374 12.68242\n1961                                    \n\n\n\nts(dt$x, start = 1960, frequency = 4)\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n1960 10.20000 10.40400 10.61208 10.82432\n1961 11.04081 11.26162 11.48686 11.71659\n1962 11.95093 12.18994 12.43374 12.68242\n1963 12.93607 13.19479 13.45868 13.72786\n1964 14.00241 14.28246 14.56811 14.85947\n\n\n\nts(dt$x, start = 1960, frequency = 1)\n\nTime Series:\nStart = 1960 \nEnd = 1979 \nFrequency = 1 \n [1] 10.20000 10.40400 10.61208 10.82432 11.04081 11.26162 11.48686 11.71659\n [9] 11.95093 12.18994 12.43374 12.68242 12.93607 13.19479 13.45868 13.72786\n[17] 14.00241 14.28246 14.56811 14.85947\n\n\n\n\n1.2.2 Creating xts Objects\nUnlike ts, xts requires an explicit time index in the form of date-like objects:\n\nyearmon (package zoo)\nyearqtr (package zoo)\nDate\nPOSIX\n\n\n# Here, we arbitrarily choose monthly measurements \n# to demonstrate the construction of xts objects\n\n# First, we construct a time index: the months from Feb 1960 to Sep 1961 by using the as.yearmon function. Note that yearmon stores a month as the year plus a fraction\n\nas.yearmon(\"1960-01\")\n\n[1] \"Jan 1960\"\n\nas.numeric(as.yearmon(\"1960-01\"))\n\n[1] 1960\n\nas.yearmon(\"1960-02\")\n\n[1] \"Feb 1960\"\n\nas.numeric(as.yearmon(\"1960-02\"))\n\n[1] 1960.083\n\nas.yearmon(\"1960-03\")\n\n[1] \"Mar 1960\"\n\nas.numeric(as.yearmon(\"1960-03\"))\n\n[1] 1960.167\n\nas.yearmon(\"1960-01\") + 1:20 / 12\n\n [1] \"Feb 1960\" \"Mar 1960\" \"Apr 1960\" \"May 1960\" \"Jun 1960\" \"Jul 1960\"\n [7] \"Aug 1960\" \"Sep 1960\" \"Oct 1960\" \"Nov 1960\" \"Dec 1960\" \"Jan 1961\"\n[13] \"Feb 1961\" \"Mar 1961\" \"Apr 1961\" \"May 1961\" \"Jun 1961\" \"Jul 1961\"\n[19] \"Aug 1961\" \"Sep 1961\"\n\ndt &lt;- dt %&gt;%\n  mutate(\n    # n() counts the number of rows in the table,\n    month = as.yearmon(\"1960-01\") + 1:n() / 12\n  )\n\ndt_xts &lt;- xts(\n  dt, \n  order.by = dt$month\n)\n\ndt_xts\n\n         tidx        x    month\nFeb 1960    1 10.20000 Feb 1960\nMar 1960    2 10.40400 Mar 1960\nApr 1960    3 10.61208 Apr 1960\nMay 1960    4 10.82432 May 1960\nJun 1960    5 11.04081 Jun 1960\nJul 1960    6 11.26162 Jul 1960\nAug 1960    7 11.48686 Aug 1960\nSep 1960    8 11.71659 Sep 1960\nOct 1960    9 11.95093 Oct 1960\nNov 1960   10 12.18994 Nov 1960\nDec 1960   11 12.43374 Dec 1960\nJan 1961   12 12.68242 Jan 1961\nFeb 1961   13 12.93607 Feb 1961\nMar 1961   14 13.19479 Mar 1961\nApr 1961   15 13.45868 Apr 1961\nMay 1961   16 13.72786 May 1961\nJun 1961   17 14.00241 Jun 1961\nJul 1961   18 14.28246 Jul 1961\nAug 1961   19 14.56811 Aug 1961\nSep 1961   20 14.85947 Sep 1961\n\n\nLike xts, tsibble requires an explicit date-like object as an index. You can use the tsibble provided functions yearmonth, yearquarter, etc.\nNote that yearmonth and yearquarter behave differently from as.yearmon and as.yearqtr. yearmonth counts the number of months since the origin. yearquarter tracks the number of quarters since the origin.\n\nyearquarter(\"2021-10\")\n\n&lt;yearquarter[1]&gt;\n[1] \"2021 Q4\"\n# Year starts on: January\n\nas.numeric(yearquarter(\"2021-10\"))\n\n[1] 207\n\n\n\ndt_ts &lt;- dt %&gt;% \n  mutate(\n    x = x, \n    month = yearmonth(\"1960-01\") + 1:n()\n    ) %&gt;%\n  as_tsibble(\n    index = month\n  )\n\ndt_ts\n\n# A tsibble: 20 x 3 [1M]\n    tidx     x    month\n   &lt;int&gt; &lt;dbl&gt;    &lt;mth&gt;\n 1     1  10.2 1960 Feb\n 2     2  10.4 1960 Mar\n 3     3  10.6 1960 Apr\n 4     4  10.8 1960 May\n 5     5  11.0 1960 Jun\n 6     6  11.3 1960 Jul\n 7     7  11.5 1960 Aug\n 8     8  11.7 1960 Sep\n 9     9  12.0 1960 Oct\n10    10  12.2 1960 Nov\n11    11  12.4 1960 Dec\n12    12  12.7 1961 Jan\n13    13  12.9 1961 Feb\n14    14  13.2 1961 Mar\n15    15  13.5 1961 Apr\n16    16  13.7 1961 May\n17    17  14.0 1961 Jun\n18    18  14.3 1961 Jul\n19    19  14.6 1961 Aug\n20    20  14.9 1961 Sep\n\n\nThe package tsbox provides functions that can convert between these classes. You can learn more about the package here.\n\nExercise 1.2 (Tsibbles) The following chunk creates tibble (dt_r) with the integers from 5 to 18 in the column “x”. These are quarterly measurements with the first observation corresponding to 2018Q2. Use mutate to add column holding a time index using the yearquarter function and create a tsibble using the as_tsibble function.\n\ndt_r &lt;- tibble(\n  x = 5:18\n)\n\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "01-Introduction.html#features-of-a-time-series",
    "href": "01-Introduction.html#features-of-a-time-series",
    "title": "1  Introduction",
    "section": "1.3 Features of a Time Series",
    "text": "1.3 Features of a Time Series\n\nTrend\nSeasonal patterns\nCyclical patterns\nNon-systematic fluctuations"
  },
  {
    "objectID": "01-Introduction.html#electricity-production-example",
    "href": "01-Introduction.html#electricity-production-example",
    "title": "1  Introduction",
    "section": "1.4 Electricity Production Example",
    "text": "1.4 Electricity Production Example\nThe dataset electr_r contains monthly values of the electricity available in the internal market (in GWh) in Bulgaria.\n\nelectr_r &lt;- read_csv(\"https://raw.githubusercontent.com/feb-sofia/ts2023/main/data/electricity/bg_internal_consumption.csv\") %&gt;%\n  mutate(\n    electr = as.numeric(electr)\n  ) %&gt;%\n  filter(!is.na(electr)) %&gt;%\n  rename(\n    GWh = electr\n  )\n\n\n# Returns the first rows so that we can get an idea about\n# the contents of the tibble (data table).\n\nhead(electr_r)\n\n# A tibble: 6 × 2\n  month     GWh\n  &lt;chr&gt;   &lt;dbl&gt;\n1 2008-01  3808\n2 2008-02  3281\n3 2008-03  2923\n4 2008-04  2593\n5 2008-05  2449\n6 2008-06  2393\n\n\nThe raw data electr_r contains a column called month that shows the month to which the value in GHw refers. The month column is character (chr) and is not in a format we can use as a time index. We need to parse the text to get a numeric representation of the month. This is what the yearmonth function does (for monthly measurements).\n\nelectr &lt;- electr_r %&gt;%\n  mutate(\n    # Create a new column called ym that holds the numeric \n    # time index\n    ym = yearmonth(month)\n  ) %&gt;%\n  as_tsibble(\n    # This assigns the newly created column ym as the time index\n    index = ym\n  )\n\n\nelectr %&gt;%\n  autoplot(GWh) + \n  labs(\n    x = \"Month\"\n  )\n\n\n\n\nAn advantage of the tsibble objects is that you can use familiar verbs, for example:\n\nmutate: to create new columns or modify existing ones\nselect: to select a subset of columns\nfilter: to select a subset of rows\n\nFor more information, check the documentation here.\n\n# Examples for creating new columns with mutate\n\nelectr1 &lt;- electr %&gt;%\n  mutate(\n    GWh_centered = GWh - mean(GWh),\n    GWh_log = log(GWh),\n  ) %&gt;%\n  select(-GWh)\n\nYou can also use the mean, sd, summary, etc. by selecting the columns from the tsibble using the $ syntax.\n\nelectr$GWh\n\n  [1] 3808 3281 2923 2593 2449 2393 2540 2560 2480 2695 3000 3320 3627 3098 3122\n [16] 2436 2311 2292 2461 2406 2304 2617 2813 3253 3496 3074 3041 2460 2303 2291\n [31] 2401 2495 2212 2721 2545 3219 3541 3176 3174 2502 2297 2363 2499 2480 2353\n [46] 2748 3161 3316 3619 3636 3121 2444 2368 2389 2602 2515 2285 2419 2781 3394\n [61] 3495 3066 3144 2639 2267 2331 2455 2455 2308 2599 2759 3378 3346 2898 2799\n [76] 2560 2431 2370 2501 2488 2375 2665 2966 3273 3491 3048 3104 2579 2306 2307\n [91] 2611 2534 2395 2631 2693 3076 3633 2872 2896 2373 2384 2381 2548 2516 2410\n[106] 2701 2933 3507 3515 2847 2618 2400 2232 2201 2393 2398 2262 2438 2683 2904\n[121] 3103 2802 2785 2037 2154 2211 2317 2361 2235 2414 2737 3216 3327 2806 2649\n[136] 2476 2350 2320 2464 2449 2283 2387 2565\n\n\n\n# Compute the arithmetic average of the values\n# in the GWh column of the data set electr\n\nmean(electr$GWh)\n\n[1] 2706.706\n\n\n\nsummary(electr$GWh)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2037    2393    2560    2707    3020    3808 \n\n\n\nelectr %&gt;%\n  gg_season() + \n  labs(\n    x = \"Month\",\n    colour = \"Year\"\n  )\n\nPlot variable not specified, automatically selected `y = GWh`\n\n\n\n\n\n\nelectr %&gt;%\n  gg_subseries() + \n  labs(\n    x = \"Year\"\n  )\n\nPlot variable not specified, automatically selected `y = GWh`\n\n\n\n\n\nBoth tsibble and xts objects support convenient filtering by the time index. You can check the syntax of both packages here:\n\nxts cheatsheet\ntsibble filter_index\n\n\nelectr %&gt;%\n  filter_index(\"2008-01\"~\"2008-12\")\n\n# A tsibble: 12 x 3 [1M]\n   month     GWh       ym\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;mth&gt;\n 1 2008-01  3808 2008 Jan\n 2 2008-02  3281 2008 Feb\n 3 2008-03  2923 2008 Mar\n 4 2008-04  2593 2008 Apr\n 5 2008-05  2449 2008 May\n 6 2008-06  2393 2008 Jun\n 7 2008-07  2540 2008 Jul\n 8 2008-08  2560 2008 Aug\n 9 2008-09  2480 2008 Sep\n10 2008-10  2695 2008 Oct\n11 2008-11  3000 2008 Nov\n12 2008-12  3320 2008 Dec\n\n\n\nelectr %&gt;%\n  filter_index(\"2008-01\"~\"2008-12\") %&gt;%\n  as_tibble() %&gt;%\n  summarise(\n    mean = mean(GWh)\n  )\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 2837.\n\n\nYou can use index_by and summarise to change the frequency of the data, i.e., to collapse the time series (e.g., from daily to monthly, from monthly to annual, etc.).\n\nelectr %&gt;%\n  index_by(qrt = ~ yearquarter(.)) %&gt;%\n  summarise(\n    GWh = sum(GWh)\n  ) %&gt;%\n  autoplot(GWh)"
  },
  {
    "objectID": "01-Introduction.html#bitcoin-exchange-rate-example",
    "href": "01-Introduction.html#bitcoin-exchange-rate-example",
    "title": "1  Introduction",
    "section": "1.5 Bitcoin Exchange Rate Example",
    "text": "1.5 Bitcoin Exchange Rate Example\n\nExercise 1.3 (Reading and Working with tsibble Objects) The following chunk uses the eurostat package to download quarterly, seasonally unadjusted series in constant 2015 prices for the Bulgarian GDP between 1995 and 2023.\n\ngdp_q &lt;- get_eurostat(\n  \"namq_10_gdp\",\n  filters = list(\n    geo=\"BG\",\n    unit = \"CLV_I15\",\n    na_item = \"B1GQ\",\n    s_adj = \"NSA\"\n  ),\n  cache = FALSE,\n  type = \"code\"\n) %&gt;%\n  filter(!is.na(values)) %&gt;%\n  mutate(\n    time = str_replace(time, \"-\", \" \"),\n    values = as.numeric(values)\n  )\n\n\nCreate a tsibble object to hold the time series. Hint: Use the as_tsibble and yearquarter functions from the tsibble package. Use the code from the introduction\nWhat is the unit of measurement: EUR or BGN? Take a look at the values and make a guess.\nWhat are the dates of the first and the last observations?\nWhat is the average quarterly GDP over the whole period.\nCompute the average GDP by quarter.\nCreate a series of annual GDP measurements by summing the values of the quarterly GDP in each year. Hint: use index_by and summarise.\nIs there a seasonal pattern in the data? Create a seasonal plot using the gg_season function from the tsibble package. Which quarter tends to have the lowest GDP?\nIs there a trend visible in the data?\nCompute the average quarterly GDP between before 2009 and after (and including) 2009.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "02-Statistics-Review.html#expectation",
    "href": "02-Statistics-Review.html#expectation",
    "title": "2  Statistics Review",
    "section": "2.1 Expectation",
    "text": "2.1 Expectation\nThe expected value of a random variable is the average of all possible values that can occurr, weighted by their occurrence probabilities. It is a measure of the location of the distribution.\n\\[\n\\begin{align}\n\\mu_x & = E(X) = \\sum_{x = 0}^{3} x p_X(x) = 0 \\times 0.250 + 1 \\times 0.095 + 2 \\times 0.272 + 3 \\times 0.383 = 1.788 \\\\\n\\end{align}\n\\]\n\nmu_x &lt;- sum(px$x * px$p)\nmu_x\n\n[1] 1.788\n\n\n\nExercise 2.2 (Expected Value) Compute the expected value of \\(Y\\).\n\n\nSolution. \n\n# Type your code here\n\n\nIf you want to predict future values of a random variable, the expected value is your best guess in the sense that it minimises the expected value of the quadratic loss function:\n\\[\nE[(X - \\hat{x})^2]\n\\]\nLet us construct an example. You need to predict the result of \\(X\\) and you think that the best prediction is \\(\\bar{x} = 1\\). When the game runs it will produce four possible values: 0, 1, 2, and 3. The error that you will make is:\n\\[\nL(x) = (x - 1)^2 =\n\\begin{cases}\n   (0 - 1)^2 = 1 & \\text{x = 0}\\\\\n   (1 - 1)^2 = 0 & \\text{x = 1}\\\\\n   (2 - 1)^2 = 1 & \\text{x = 2}\\\\\n   (3 - 1)^2 = 4 & \\text{x = 3}\n\\end{cases}\n\\]\n\nExercise 2.3 (Expected Quadratic Loss) Compute the expected quadratic loss for a prediction \\(\\bar{x} = 1.5\\).\n\n\nSolution. \n\n## Type your code here\n\n# px %&gt;%\n#   mutate(\n#     loss = ?\n#   )"
  },
  {
    "objectID": "02-Statistics-Review.html#variance",
    "href": "02-Statistics-Review.html#variance",
    "title": "2  Statistics Review",
    "section": "2.2 Variance",
    "text": "2.2 Variance\nThe variance of a random variable measures how different the possible values that can occur are. Values that occur more often (have higher probability) under \\(p_X\\) receive a higher weight. Values that occur less frequently under \\(p_X\\) are given a lower weight in the sum.\n\\[\n\\begin{align}\nVar(X) & = \\sum_{x = 0}^{3} (x - \\mu_x)^2 \\times p_X(x) \\\\\n       & = (0 - 1.788)^2 \\times 0.250 + (1 - 1.788)^2 \\times 0.095 + (2 - 1.788)^2\\times 0.272 + (3 - 1.788)^2 \\times 0.383 \\\\\n       & = (-1.788)^2 \\times 0.250 + (-0.788)^2 \\times 0.095 + (0.212)^2\\times 0.272 + (1.212)^2 \\times 0.383 \\\\\n       & = 3.196 \\times 0.250 + 0.620^2 \\times 0.095 + 0.044 \\times 0.272 + 1.468 \\times 0.383 \\\\\n       & \\approx 1.433\n\\end{align}\n\\tag{2.1}\\]\n\n(px$x - mu_x)\n\n[1] -1.788 -0.788  0.212  1.212\n\n(px$x - mu_x)^2\n\n[1] 3.196944 0.620944 0.044944 1.468944\n\npx$p * (px$x - mu_x)^2\n\n[1] 0.79923600 0.05898968 0.01222477 0.56260555\n\nsum(px$p * (px$x - mu_x)^2)\n\n[1] 1.433056\n\n\nYou can see from Equation 2.1 that it is the expected value of the squared diviations from the expected value.\n\\[\nVar(X) = E(X - E(X))^2\n\\]\n\nExercise 2.4 (Variance) Compute the Variance of \\(Y\\).\n\n\nSolution. \n\n# Type your code here\n\n\n\nTheorem 2.1 (Properties of the Expectation) Let \\(X\\) be a random variable with expected value \\(E(X)\\), let \\(Y\\) be a random variable with expected value \\(E(Y)\\), and let \\(a\\) be a fixed constant (\\(a \\in \\mathbb{R}\\)). The following properties are true:\n\\[\n\\begin{align}\nE(a) & = a \\\\\nE(aX) & = aE(X) \\\\\nE(X + Y) & = E(X) + E(Y)\n\\end{align}\n\\]\nFurthermore, if \\(X\\) and \\(Y\\) are uncorrelated, then the expected value of the product of the two random variables equals the product of their expected values:\n\\[\nE(XY) = E(X)E(Y)\n\\]\n\n\nTheorem 2.2 (Properties of the Variance) Let \\(X\\) be a random variable with expected value \\(E(X)\\), let \\(Y\\) be a random variable with expected value \\(E(Y)\\), and let \\(a\\) be a fixed constant (\\(a \\in \\mathbb{R}\\)). The following properties are true:\n\\[\nVar(X) = E(X^2) - E(X)^2\n\\]\n$$ \\[\\begin{align}\nVar(a) & = 0 \\\\\n\nVar(aX) & = a^2 Var(X)\n\\end{align}\\] $$ Furthermore, if \\(X\\) and \\(Y\\) are uncorrelated, then the variance of their sum equals the sum of their variances:\n\\[\nVar(X + Y) = Var(X) + Var(Y)\n\\]\n\n\nExercise 2.5 (Expected value and variance) Use the distributions of \\(X\\) and \\(Y\\) from Table 2.1 and Table 2.2 to compute the expected value and the variance of\n\\[\n2X + 3Y + 1.\n\\]\nAssume that \\(X\\) and \\(Y\\) are independent.\n\n\nSolution. \\[\nE(2X + 3Y + 1) = \\\\\nVar(2X + 3Y + 1) =\n\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#joint-distribution",
    "href": "02-Statistics-Review.html#joint-distribution",
    "title": "2  Statistics Review",
    "section": "2.3 Joint Distribution",
    "text": "2.3 Joint Distribution\nIt is as summary of the joint distribution of \\(X\\) and \\(Y\\). The joint probability mass function tells you the probability of the simultaneous occurrence of \\(x\\) and \\(y\\). For example, you can ask it the question: what is the probability of \\(x = 2\\) and \\(y = 3\\).\nFor two discrete variables, it is convenient to present the joint distribution as a table with cell entries holding the probabilities. The joint distribution is given in the tibble pxy in a long format.\n\npxy %&gt;%\n  knitr::kable()\n\n\n\n\nx\ny\np\n\n\n\n\n0\n2\n0.241\n\n\n0\n3\n0.009\n\n\n1\n2\n0.089\n\n\n1\n3\n0.006\n\n\n2\n2\n0.229\n\n\n2\n3\n0.043\n\n\n3\n2\n0.201\n\n\n3\n3\n0.182\n\n\n\n\n\nSometimes it is more convenient to see this distribution in a wide format:\n\npxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix=\"y=\"\n    ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.3: Joint distribution of \\(X\\) and \\(Y\\).\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.241\n0.009\n\n\n1\n0.089\n0.006\n\n\n2\n0.229\n0.043\n\n\n3\n0.201\n0.182\n\n\n\n\n\n\n\\[\np_{XY}(x=2, y=3) = 0.043\n\\]\nThe joint probability distribution function must sum (integrate) to one over all possible pairs of \\(x\\) and \\(y\\).\n\\[\n\\sum_{x = 0}^{3}\\sum_{y = 2}^{3} p_{XY}(x, y) = 1\n\\]\n\nsum(pxy$p)\n\n[1] 1\n\n\nIn the example until now we have summarized the marginal distributions of \\(X\\) and \\(Y\\) but we have said nothing about their joint distribution. Usually the joint distribution is determined by the subject matter at hand, but for the sake of example we will look at two joint distributions so that we can get an idea how they work.\nFirst we will construct a special joint distribution under the assumption of independence. Intuitively, two random variables are independent, if the outcome of one of the variables does not influence the probability distribution of the other. Imagine that you hold two lottery tickets: one from a lottery in Germany and another from a lottery in Bulgaria. It would be safe to assume that the realized winnings from the German lottery will not affect the odds to win from the Bulgarian ticket.\nNow let us consider a case of dependent random variables. Let \\(X\\) be the level of a river (at some measurement point) at time \\(t\\) and \\(Y\\) be the level of the same river five minutes later. It woule be safe to assume that if the level of the river was high at \\(t\\) this would affect the distribution of the level of the river at \\(t\\) plus five minutes."
  },
  {
    "objectID": "02-Statistics-Review.html#marginal-distributions",
    "href": "02-Statistics-Review.html#marginal-distributions",
    "title": "2  Statistics Review",
    "section": "2.4 Marginal Distributions",
    "text": "2.4 Marginal Distributions\nThe marginal distribution of \\(X\\) is obtained by summing the joint distribution of \\(X\\) and \\(Y\\) over all possible values of \\(Y\\).\n\\[\np_X(x) = \\sum_{y=2}^{3}p_{XY}(x, y)\n\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#conditional-distributions",
    "href": "02-Statistics-Review.html#conditional-distributions",
    "title": "2  Statistics Review",
    "section": "2.5 Conditional Distributions",
    "text": "2.5 Conditional Distributions\n\npxy_w &lt;- pxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  ) %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  )\n\npxy_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.4: Conditional distributions of \\(Y\\) given \\(X\\)\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.964\n0.036\n0.250\n\n\n1\n0.937\n0.063\n0.095\n\n\n2\n0.842\n0.158\n0.272\n\n\n3\n0.525\n0.475\n0.383\n\n\n\n\n\n\nLooking at the conditional distributions of \\(Y\\) given \\(X\\) in Table 2.4, you should notice that these are not the same for each value of \\(X\\). For example, \\(Y=2\\) is much more likely when \\(X = 0\\) compared to \\(X = 3\\)."
  },
  {
    "objectID": "02-Statistics-Review.html#joint-distribution-under-independence",
    "href": "02-Statistics-Review.html#joint-distribution-under-independence",
    "title": "2  Statistics Review",
    "section": "2.6 Joint Distribution under Independence",
    "text": "2.6 Joint Distribution under Independence\nLets construct the joint distribution \\(p_{XY}(x, y)\\) that assigns a probability to the points \\((x, y)\\), assuming that \\(X\\) and \\(Y\\) are independent.\nFor independent random variables the joint probability of occurrence is simply the product of the marginal distributions.\n\\[\np_{XY}(x, y) = p_X(x)p_Y(y)\n\\]\n\npxy_ind &lt;- expand_grid(\n  px %&gt;% rename(p_x = p), \n  py %&gt;% rename(p_y = p)\n)\npxy_ind &lt;- pxy_ind %&gt;%\n  mutate(\n    p = p_x * p_y\n  )\n\n\npxy_ind_w &lt;- pxy_ind %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  )\n\npxy_ind_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.5: Joint distribution of \\(X\\) and \\(Y\\) under independence.\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.190\n0.060\n\n\n1\n0.072\n0.023\n\n\n2\n0.207\n0.065\n\n\n3\n0.291\n0.092\n\n\n\n\n\n\nLet’s look at the conditional distributions of \\(Y\\) given \\(X\\). These answer the questions of the type: if \\(X\\) turns out to be \\(0\\), what are the probabilities for \\(Y = 2\\) and \\(Y = 3\\).\nTo get the conditional distributions of Y for each possible value of \\(X\\) we divide the cells of the joint distribution table by the marginal probabilities of each \\(x\\).\n\\[\np_{Y|X}(x, y) = \\frac{p_{XY}(x, y)}{p_X(x)}\n\\]\n\npxy_ind_w %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.6: Conditional distributions of \\(Y\\). Independence case.\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.76\n0.24\n0.250\n\n\n1\n0.76\n0.24\n0.095\n\n\n2\n0.76\n0.24\n0.272\n\n\n3\n0.76\n0.24\n0.383\n\n\n\n\n\n\nWhat you should see in Table 2.6 is that the conditional distributions of \\(Y\\) are the same for every possible value of \\(X\\). This is of course a consequence of the way we constructed this joint distribution in the first place: namely, we assumed that \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "02-Statistics-Review.html#conditional-expectation",
    "href": "02-Statistics-Review.html#conditional-expectation",
    "title": "2  Statistics Review",
    "section": "2.7 Conditional Expectation",
    "text": "2.7 Conditional Expectation\nWe have seen how we derived the conditional distributions of \\(Y\\) given \\(X\\) in the previous section. Now we can ask the question: what is the expected value of \\(Y\\) given that \\(X\\) has already turned out to be 0 (for example). We can take the conditional distribution of \\(Y\\) given \\(X = 0\\) and compute the expected value of this distribution.\nFor the joint distribution under independence:\n\\[\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.76 + 3 \\times 0.24 = 2.24\n\\]\n\n2 * 0.76 + 3 * 0.24\n\n[1] 2.24\n\n\nFor the joint distribution in Table 2.4 the conditional expectation of \\(Y\\) given \\(X = 0\\) is\n\\[\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.964 + 3 \\times 0.036 = 2.036\n\\]\n\n2 * 0.964 + 3 * 0.036\n\n[1] 2.036\n\n\nLet us write the conditional expectation of \\(Y\\) for each possible value of \\(X\\) for the dependent joint distribution case.\n\\[\nE(Y | X = x) = \\begin{cases}\n  2.036 & \\text{for } x = 0 \\\\\n  2.060 & \\text{for } x = 1 \\\\\n  2.158 & \\text{for } x = 2 \\\\\n  2.475 & \\text{for } x = 3\n\\end{cases}\n\\]\n\npxy %&gt;%\n  group_by(x) %&gt;%\n  summarise(\n    y = y,\n    p_y_x = p / sum(p)\n  ) %&gt;%\n  summarise(\n    E_Y_given_X = sum(y * p_y_x) \n  ) %&gt;%\n  knitr::kable(digits = 3)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'x'. You can override using the `.groups`\nargument.\n\n\n\n\nTable 2.7: Conditional expectation of \\(Y\\) for each possible value of \\(X\\).\n\n\nx\nE_Y_given_X\n\n\n\n\n0\n2.036\n\n\n1\n2.063\n\n\n2\n2.158\n\n\n3\n2.475\n\n\n\n\n\n\nAn important thing to see here is that the conditional expectation is different for each value of \\(X\\). As the value of \\(X\\) is uncertain (it is a random variable), the conditional expectation of \\(Y\\) given \\(X\\) is also a random variable. Its distribution is given by the possible values and the probabilities of occurrence of \\(X\\) (the marginal distribution of \\(X\\)).\n\nExercise 2.6 Calculate the expected value of \\(Y\\) given \\(X\\) for every possible value of \\(X\\) in the case joint distribution under independence.\n\n\nExample 2.2 (Sampling from the Joint Distribution)  \n\nsample_joint &lt;- pxy %&gt;%\n  slice_sample(n = 1000, weight_by = p, replace = TRUE)\n\nhead(sample_joint)\n\n# A tibble: 6 × 3\n      x     y     p\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     3     2 0.201\n2     0     2 0.241\n3     3     2 0.201\n4     3     2 0.201\n5     3     3 0.182\n6     2     2 0.229\n\n\n\nsample_joint %&gt;%\n  group_by(x, y) %&gt;%\n  summarise(\n    p = first(p),\n    n = n(),\n    f = n / nrow(sample_joint)\n  )\n\n`summarise()` has grouped output by 'x'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 5\n# Groups:   x [4]\n      x     y     p     n     f\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1     0     2 0.241   243 0.243\n2     0     3 0.009    10 0.01 \n3     1     2 0.089    85 0.085\n4     1     3 0.006     5 0.005\n5     2     2 0.229   226 0.226\n6     2     3 0.043    43 0.043\n7     3     2 0.201   199 0.199\n8     3     3 0.182   189 0.189"
  },
  {
    "objectID": "02-Statistics-Review.html#covariance",
    "href": "02-Statistics-Review.html#covariance",
    "title": "2  Statistics Review",
    "section": "2.8 Covariance",
    "text": "2.8 Covariance\nThe covariance measures the (linear) dependency between two random variables.\n\nDefinition 2.1 (Covariance) The covariance of two random variables \\(X\\) and \\(Y\\) is given by\n\\[\nCov(X, Y) = E[(X - E(X))(Y - E(Y))]\n\\] Alternatively, it can be computed using the decomposition formula:\n\\[\nCov(X, Y) = E(XY) - E(X)E(Y)\n\\]\n\nIn the analysis of time series we will often encounter situations where the expected value of one of the random variables is zero. As can be seen from the decomposition formula, in that case the covariance reduces to\n\\[\nCov(X, Y) = E(XY).\n\\]\nClosely related to the covariance is the correlation between \\(X\\) and \\(Y\\).\n\nDefinition 2.2 (Correlation) \\[\n\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}\n\\] It is easy to show that the correlation is bounded between -1 and 1.\n\\[\n-1 \\leq \\rho(X, Y) \\leq 1\n\\]\n\n\nExercise 2.7 (Correlation) Let X be a random variable with, and \\(Y = a + bX\\). Show that the correlation between \\(X\\) and \\(Y\\) equals one or minus one depending on the sign of \\(b\\). For simplicity, assume that \\(E(X) = 0\\).\n\n\nTheorem 2.3 (Properties of the Covariance) Let \\(X\\) and \\(Y\\) be random variables and let \\(a, b \\in \\mathbb{R}\\) be fixed constants.\n\\[\nVar(aX + bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X, Y)\n\\]\n\n\nExercise 2.8 (Covariance) Compute the covariance of \\(X\\) and \\(Y\\) under the joint distributions given in Table 2.5 and Table 2.3. Use the pxy and pxy_ind tables for these calculations.\n\n\nSolution. \n\n# Type your code here\n\n\n\nExercise 2.9 (Variance of Correlated Variables) Compute the variance of \\(2X - Y\\) using the same distributions as in Exercise 2.8."
  },
  {
    "objectID": "02-Statistics-Review.html#empirical-estimation-of-moments",
    "href": "02-Statistics-Review.html#empirical-estimation-of-moments",
    "title": "2  Statistics Review",
    "section": "2.9 Empirical Estimation of Moments",
    "text": "2.9 Empirical Estimation of Moments\nThroughout this course we will focus on three summaries of the stochastic processes: the mean (expected value, level), the variance (fluctuation), and the covariances/correlations (dependency) between lags of the random process.\nIn the the previous statistics-related courses we had a sample of (uncorrelated) observations \\(x_1, x_2,\\ldots, x_n\\). We assumed that these are samples (realizations) from some normal distribution \\(N(\\mu, \\sigma^2)\\) with unknown mean and variance. We used the observed values to learn something about this distribution. We estimated its mean with\n\\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\\]\nand its variance with:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n  -1}\\sum_{i = 1}^{n}(x_i - \\hat{\\mu})^2\n\\tag{2.2}\\]\nIn the case of time series, however, we have only one observation for each time point.\n\nn &lt;- 10\nB &lt;- 5\nset.seed(21)\n\ndt_rw &lt;- tibble(\n  r = rep(1:B, n),\n  e = rnorm(n = B*n)\n) %&gt;%\n  group_by(r) %&gt;%\n  mutate(\n    t = 1:n,\n    x = cumsum(e),\n    is_observed = (r == 1),\n  )\n\ndt_rw %&gt;%\n  ggplot(\n    aes(\n      x = t,\n      y = x, \n      group = r, \n      alpha = factor(is_observed))\n    ) +\n  geom_line() + \n  geom_point() + \n  labs(\n    alpha = \"Observed\",\n    y = expression(x[t])\n  ) + \n  scale_x_continuous(breaks = 1:n)\n\n\n\n\nIn order to be able to apply our usual technique for moments estimation we have to assume that the random process is ergodic, meaning that we can estimate its statistical properties (mean, variance, correlations) from the its time series (observed values). We will talk more extensively about (weak) stationarity, which is a necessary (though not sufficient) condition for ergodicity. For (weakly) stationary time series the mean, variance and covariances do not change over time.\n\\[\n\\begin{align}\n\\mu & = E(X_1) = E(X_2) = \\ldots = E(X_T) \\\\\n\\gamma(0) & = Var(X_1) = Var(X_2) = \\ldots = Var(X_T) \\\\\n\\gamma(1) & = Cov(X_1, X_2) = Cov(X_2, X_3) = \\ldots = Cov(X_{t - 1}, X_{t}) = Cov(X_{T - 1}, X_{T}) \\\\\n\\gamma(k) & = Cov(X_{t}, X_{t + k}) \\\\\n\\rho(k) & = \\frac{\\gamma(k)}{\\gamma(0)}\n\\end{align}\n\\]\nFor a random process \\(X_1, X_2,\\ldots, X_T\\) and an observed time series \\(x_1, x_2, \\ldots, x_T\\), we will estimate the mean with the arithmetic average of the observed values.\n\\[\n\\hat{\\mu} = \\frac{1}{T}\\sum_{t = 1}^{T} x_{t}\n\\]\nWe will estimate the variance of a stationary process with the empirical variance:\n\\[\n\\hat{\\gamma}(0) = \\frac{1}{T}\\sum_{t = 1}^{T}(x_t - \\bar{x})^2\n\\tag{2.3}\\]\nAnd the covariances with the empirical (auto-)covariances:\n\\[\n\\hat{\\gamma}(k) = \\frac{1}{T}\\sum_{t = 1}^{T - k} (x_{t} - \\hat{\\mu})(x_{t + k} - \\hat{\\mu})\n\\tag{2.4}\\]\nand empirical (auto-)correlations:\n\\[\n\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)}\n\\] To see the meaning of the autocovariances, let us create two artificial time series and plot their lags. The time series will be generated from two processes. The first one will be called the purely random or white noise process.\n\\[\ne_t \\sim N(0, 1) \\text{ for every } t \\\\\nE(e_t) = 0 \\\\\nVar(e_t) = 1 \\\\\nCov(e_t, e_{t + k}) = 0, k \\neq 0\n\\] The second one is be called a random walk.\n\\[\ny_t = \\sum_{i = 1}^{t} e_i\n\\]\n\nset.seed(432)\n\n# Create 20 observations from two time series. rnorm draws random numbers\n# from a normal distribution with zero expected value (mean) and standard\n# deviation (sd) equal to 1. We call this distribution the standard normal\n# distribution. In the mutate step we create the first two lags of the series\n\ndt_sim &lt;- tibble(\n  x = rnorm(n = 20, mean = 0, sd = 1),\n  y = cumsum(x)\n) %&gt;%\n  mutate(\n    x_l1 = lag(x, n = 1),\n    y_l1 = lag(y, n = 1)\n  )\n\nmean_x &lt;- mean(dt_sim$x)\nmean_x\n\n[1] -0.08259397\n\nmean_y &lt;- mean(dt_sim$y)\nmean_y\n\n[1] 0.8611093\n\n\n\ndt_sim %&gt;%\n  ggplot(aes(x = x, y = x_l1)) +\n  geom_point() + \n  geom_vline(xintercept=mean_x, lty=2, alpha = 0.5) + \n  geom_hline(yintercept=mean_x, lty=2, alpha = 0.5) + \n  labs(\n    x = \"x\",\n    y = \"lag(x, 1)\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nLooking at the plot, think about the signs of the cross-products that go into the formula for the covariance:\n\\[\n(x_{t + 1} - \\hat{\\mu})(x_{t} - \\hat{\\mu})\n\\]\nWhere do you find positive and negative cross-products in the plot?\n\ndt_sim %&gt;%\n  ggplot(aes(x = y_l1, y = y)) +\n  geom_point() + \n  geom_vline(xintercept=mean_y, lty=2, alpha = 0.5) + \n  geom_hline(yintercept=mean_y, lty=2, alpha = 0.5) + \n  labs(\n    x = \"lag(y, 1)\",\n    y = \"y\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe following code computes the covariance\n\n# Compute the covariance\ncov(dt_sim$x, dt_sim$x_l1, use = \"complete.obs\")\n\n[1] -0.1063495\n\n# COmpute the correlation\ncor(dt_sim$x, dt_sim$x_l1, use = \"complete.obs\")\n\n[1] -0.1821236\n\n\nNote that cov computes the covariance according to formula Equation 2.2."
  },
  {
    "objectID": "02-Statistics-Review.html#exercises",
    "href": "02-Statistics-Review.html#exercises",
    "title": "2  Statistics Review",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\nThe following code downloads\n\nbitcoin &lt;- getSymbols(\n  \"BTC-USD\", \n  from=\"2015-01-01\",\n  to=\"2023-01-01\",\n  src=\"yahoo\", \n  periodicity=\"daily\", \n  env = NULL)\n\nnames(bitcoin) &lt;- c(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adjusted\")\n\nbc &lt;- bitcoin %&gt;%\n  ts_tsibble() %&gt;%\n  pivot_wider(\n    names_from = id,\n    values_from = value\n  )\n\n\n# Plots the closing exchange rate against the time index\n\nbc %&gt;%\n  autoplot(Close) + \n  labs(\n    # Set the labels for the x and y axis\n    x = \"Date\",\n    y = \"USD/BTC\"\n  )\n\n\n\n\n\nCompute the average exchange rate (Close).\n\n\n# Type your code here\n\n\nCompute the empirical variance using the var function\n\n\n# Type your code here\n\n\nConstruct a two new variables that contain the first and the second lags of Close. Leave only Close and its lags in the resulting tsibble. Hint: Use the mutate and lag functions and select the columns using select.\n\n\n# Type your code here\n# Select the code below and press Ctrl-Shift-c in order to remove the comments\n\n\nCreate a scatterplot for Close and its first lag. Hint: Use the code generating the scatterplot in the simulation above and adapt it.\n\n\n# Type your code here\n\n\nCompute the covariance between Close and its first lag. Compute the autocorrelation coefficient (empirical autocorrelation) between Close and its first lag. Hint: Look into the documentation of cov and cor to find out how to handle missing values.\n\n\n# Type your code here\n\n\nUse the gg_lag function to create scatterplots for the between the time series and its first six lags. What patterns can you see in these scatterplots?\n\n\nbc %&gt;%\n  gg_lag(\n    Close, \n    lags = 1:6, \n    geom=\"point\", \n    size = 0.5\n  )\n\n\n\n\n\nUse the acf function to plot the autocorrelation function of the Close time series.\n\n\n# Type your code here"
  },
  {
    "objectID": "03-linear-difference-equations.html#first-order-difference-equations",
    "href": "03-linear-difference-equations.html#first-order-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.1 First Order Difference Equations",
    "text": "3.1 First Order Difference Equations\nDifference equations describe the dynamic behavior of a sequence by relating the current value of a variable to its previous values.\nLet us start with a simple difference equation defined by:\n\\[\ny_{t} = \\phi y_{t - 1} + e_t\n\\]\nFor example, imagine that it describes the value of your bank deposit. Suppose you receive an interest of 2 percent annually. In that case, the value of your account this year (\\(y_t\\)) is equal to the value of the account the previous year plus the interest earned (\\(\\phi = 1 + 0.02\\)). You can think about \\(e_t\\) as withdrawals or additional deposits to your account.\nWe want to achieve two goals. The first goal is to solve this equation by expressing \\(y_t\\) as a function of some initial value \\(y_0\\), the time index \\(t\\), the parameter \\(\\phi\\) and the sequence of \\(e_t\\).\nIt helps if we write the equation for a couple of periods.\n\\[\n\\begin{align}\n1 & \\quad y_1 = \\phi y_{0} + e_{1} \\\\\n2 & \\quad y_2 = \\phi y_{1} + e_{2} \\\\\n2 & \\quad y_3 = \\phi y_{2} + e_{3} \\\\\n& \\vdots \\\\\nt & \\quad y_{t} = \\phi y_{t - 1} + e_{t} \\\\\nt + 1 &  \\quad y_{t + 1} = \\phi y_{t} + e_{t + 1}\\\\\nt + 2 & \\quad y_{t + 2} = \\phi y_{t + 1} + e_{t + 2}\n\\end{align}\n\\]\nTry substituting the first equation into the second, then the second into the third equation.\n\n\n\n\n\n\nClick here to see the solution\n\n\n\n\n\n\\[\ny_3 = \\phi^3 y_0 + \\phi^2 e_{1} + \\phi e_{2} + \\phi^0e_{3}\n\\]\n\n\n\nYou can generalize this for \\(y_t\\):\n\\[\ny_{t + j} = \\phi^{j + 1} y_{t - 1} + \\phi^{j}e_{t} + \\phi^{j - 1} e_{t + 1} + \\ldots \\phi^1 e_{t + j - 1} + \\phi^0 e_{t + j}\n\\] We can write this more compactly as:\n\\[\ny_{t + j} = \\phi^{j + 1} y_{t - 1} + \\sum_{k = 0}^{j} \\phi^{k} e_{t + j - k}\n\\]\nYou should check that the above formula is correct. Let’s write it down for \\(j = 2\\).\n\n\n\n\n\n\nClick here to see the result\n\n\n\n\n\n\\[\n\\begin{align}\ny_{t + 2} = \\phi^{2 + 1}y_{t - 1} & + \\phi^0 e_{t + 2 - 0} \\\\\n& + \\phi^{1} e_{t + 2 - 1} \\\\\n& + \\phi^{2} e_{t + 2 - 2} \\\\\n\\end{align}\n\\]\n\n\n\nNow that we know how to solve the equation, we can analyze the behavior of the dynamic system that it describes. It turns out that this behavior is determined by the parameter \\(\\phi\\).\nLet’s investigate the effect of a unit change in \\(e_t\\) on \\(y_{t + j}\\), assuming that all other values of \\(e\\) remain unchanged.\n\\[\n\\frac{\\partial y_{t + j}}{\\partial e_{t}} = \\phi^j\n\\] Note that the effect on \\(y_{t}\\) only depends on the distance in time between the shock \\(e_t\\) and the response \\(y_{t + j}\\), not on the time index itself. This is a property of linear difference equations.\nNow, obviously, a unit shock in time \\(t\\) will have a different effect on \\(y_{t + j}\\) depending on the value of \\(\\phi\\).\nAnother effect that we would like to analyze is the effect of a permanent unit change in the autonomous process \\(e_t\\). What will happen to \\(y_t\\) if all \\(e_t\\) increase by one unit?\n\n\n\n\n\n\nClick here to see the result\n\n\n\n\n\n\\[\ny_{t + j} = \\phi^{j + 1}y_{t - 1} + \\sum_{k = 0}^{j}\\phi^k e_{t + j - k}\n\\]\n\\[\n\\begin{align}\ny^{*}_{t + j} & = \\phi^{j + 1}y_{t - 1} & + \\sum_{k = 0}^{j}\\phi^k (e_{t + j - k} + 1) \\\\\n              & = \\phi^{j + 1}y_{t - 1} & + \\sum_{k = 0}^{j}\\phi^k e_{t + j - k} + \\sum_{k = 0}^{j}\\phi^k\n\\end{align}\n\\]\nFrom this, it should be easy to see that\n\\[\ny^{*}_{t + j} - y_{t + j} = \\sum_{k = 0}^{j}\\phi^k.\n\\]\n\n\n\nThe effect of a permanent unit change in \\(e_t\\) is thus a power series in the parameter \\(\\phi\\). At this point, it is helpful to remember a theorem from school about the limit of this series.\n\nTheorem 3.1 (Convergence of a Geometric Series) \\[\nS_{n} = 1 + \\phi + \\phi^2 + \\phi^3 + \\ldots + \\phi^n\n\\]\nAssuming that \\(\\phi \\neq 1\\), this series can be expressed more compactly as\n\\[\nS_{n} = \\frac{1 - \\phi^{n + 1}}{1 - \\phi}\n\\]\nTo see this, compute the difference between \\(S_n\\) and \\(\\phi S_n\\) and rearrange.\nIf \\(|\\phi| &lt; 1 \\iff -1 &lt; \\phi &lt; 1\\) the series has a limit for \\(n \\to \\infty\\):\n\\[\n\\lim_{n \\to \\infty} S_{n} = \\frac{1}{1 - \\phi}\n\\]\n\n\nExercise 3.1 (Geometric Series) To see how this series behaves, give it a try in R.\n\nphi &lt;- 0.5\n# This will give you a vector of the first 11 elements\n\ndt_gseries &lt;- tibble(\n  n = 0:10,\n  el = phi^n,\n# The geometric series is simply the cumulative sum of these elements\n  series = cumsum(el)\n)\n\ndt_gseries %&gt;%\n  ggplot(aes(x = n, y = series)) + \n  geom_line()\n\n\n\n\n\nLet us check\n\nset.seed(321)\n\n# Set the parameter phi\nphi &lt;- 0.5\n# Set the initial value (y_{t - 1}) in the above notation\ny_init &lt;- 1\n# Set the number of periods j\nB &lt;- 1\n\ndt &lt;- tibble(\n  j = 0:B,\n  # Generate random values for e\n  e = rnorm(n = B + 1),\n  # Add one to the first row of e\n  e_trans = e + c(1, rep(0, B)),\n  # Add one to all values of e\n  e_perm = e + 1,\n  # Compute the powers of phi used to weight e/e_trans/e_perm\n  phi_powers = phi^(B - j)\n)\n\ndt %&gt;%\n  summarise(\n    # The following simply implements the formula for y\n    y = phi^(B + 1) * y_init + sum(e * phi_powers),\n    y_trans = phi^(B + 1) * y_init + sum(e_trans * phi_powers),\n    y_perm = phi^(B + 1) * y_init + sum(e_perm * phi_powers),\n    y_diff_trans = y - y_trans,\n    y_diff_perm = y - y_perm\n  ) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\n\ny\ny_trans\ny_perm\ny_diff_trans\ny_diff_perm\n\n\n\n\n0.39\n0.89\n1.89\n-0.5\n-1.5\n\n\n\n\n\n\nExercise 3.2 Given a first order difference equation with \\(\\phi = 0.8\\), find the value of the process in \\(t = 3\\), knowing that it started with \\(y_0 = 5\\). Assume that \\(e_t = 0\\) for all \\(t\\).\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "03-linear-difference-equations.html#first-order-vector-difference-equations",
    "href": "03-linear-difference-equations.html#first-order-vector-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.2 First-Order Vector Difference Equations",
    "text": "3.2 First-Order Vector Difference Equations\nIn this course, we will focus exclusively on univariate time series analysis. Still, for the sake of motivating the study of higher-order linear difference equations, it is convenient to introduce vector linear difference equations. Let’s start with two variables, \\(x_t\\) and \\(y_t\\).\n\\[\n\\begin{align}\nx_{t} & = \\phi_{x,1}x_{t - 1} + \\phi_{y, 1}y_{t - 1} + e_{x,t} \\\\\ny_{t} & = \\phi_{x_2}x_{t - 1} + \\phi_{y, 2}y_{t - 1} + e_{y,t}\n\\end{align}\n\\]\nIt is convenient to write this equation in matrix form.\n\\[\n\\underbrace{\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}}_{\\mathbf{z}_t}\n= \\underbrace{\\begin{pmatrix}\n\\phi_{x, 1} & \\phi_{y, 1} \\\\\n\\phi_{x, 2} & \\phi_{y, 2}\n\\end{pmatrix} }_{\\mathbf{A}}\n\\underbrace{\\begin{pmatrix}\nx_{t - 1} \\\\\ny_{t - 1}\n\\end{pmatrix}}_{\\mathbf{z}_{t - 1}} +\n\\underbrace{\\begin{pmatrix}\ne_{x,t} \\\\\ne_{y,t}\n\\end{pmatrix}}_{\\mathbf{e}_t}\n\\]\n\\[\n\\mathbf{z}_{t} = \\mathbf{A} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t}\n\\]\nIf \\(\\mathbf{A}\\) were a diagonal matrix, then we already know the solution because we can solve each equation separately.\n\\[\n\\underbrace{\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}}_{\\mathbf{z}_t}\n= \\underbrace{\\begin{pmatrix}\n\\phi_{x,1} & 0 \\\\\n0 & \\phi_{y, 2}\n\\end{pmatrix} }_{\\mathbf{A}}\n\\underbrace{\\begin{pmatrix}\nx_{t - 1} \\\\\ny_{t - 1}\n\\end{pmatrix}}_{\\mathbf{z}_{t - 1}} +\n\\underbrace{\\begin{pmatrix}\ne_{x,t} \\\\\ne_{y,t}\n\\end{pmatrix}}_{\\mathbf{e}_t}\n\\]\nHowever, \\(\\mathbf{A}\\) depends on the subject matter at hand, and we need to be able to handle the general case (not diagonal). Linear algebra provides an elegant solution to our problem. The eigendecomposition of the matrix \\(\\mathbf{A}\\) allows us to transform the system of equations into a form where we can apply the simple solution. After solving the equations, we can back-transform to the original variables.\n\\[\n\\mathbf{A} = \\mathbf{V}\n\\begin{pmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{pmatrix}\n\\mathbf{V}^{-1}\n\\]\nUsing this decomposition, we can transform the complicated problem to a simple one with only diagonal entries in the matrix before \\(\\mathbf{z}_{t - 1}\\).\n\\[\n\\begin{align}\n\\mathbf{z}_{t} & = \\mathbf{A} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t} \\\\\n\\mathbf{z}_{t} & = \\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V^{-1}} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t} \\\\\n\\mathbf{V}^{-1} \\mathbf{z}_{t} & = \\mathbf{V}^{-1} \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\mathbf{z}_{t - 1} + \\mathbf{V}^{-1}\\mathbf{e}_{t} \\\\\n\\mathbf{V}^{-1} \\mathbf{z}_{t} & = \\mathbf{\\Lambda}\\mathbf{V}^{-1} \\mathbf{z}_{t - 1} + \\mathbf{V}^{-1}\\mathbf{e}_{t} \\\\\n\\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}_{t}}\n\\end{align}\n\\]\nLets see an example.\n\nB &lt;- matrix(c(1, 9, 4, 1), ncol = 2)\nB\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    9    1\n\n\n\ned &lt;- eigen(B)\ned\n\neigen() decomposition\n$values\n[1]  7 -5\n\n$vectors\n          [,1]       [,2]\n[1,] 0.5547002 -0.5547002\n[2,] 0.8320503  0.8320503\n\n\n\nsolve(ed$vectors)\n\n           [,1]      [,2]\n[1,]  0.9013878 0.6009252\n[2,] -0.9013878 0.6009252\n\n\n\ned$vectors %*% diag(ed$values) %*% solve(ed$vectors)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    9    1\n\n\nIn this example the diagonal matrix \\(\\mathbf{\\Lambda}\\) is equal to\n\\[\n\\mathbf{\\Lambda} = \\begin{pmatrix}\n7 & 0 \\\\\n0 & -5\n\\end{pmatrix}\n\\] The inverse of the eigenvectors matrix is\n\\[\n\\mathbf{V}^{-1} = \\begin{pmatrix}\n0.9 & 0.6 \\\\\n-0.9 & 0.6\n\\end{pmatrix}\n\\]\nThe transformed vectors \\(\\tilde{\\mathbf{z}_{t}}\\) will look like:\n\\[\n\\tilde{\\mathbf{z}_{t}} = \\mathbf{V}^{-1}\\mathbf{z}_t = \\begin{pmatrix}\n0.9 & 0.6 \\\\\n-0.9 & 0.6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.9x_t + 0.6y_t \\\\\n-0.9x_t + 0.6 y_t\n\\end{pmatrix}\n\\]\nThe whole system looks like:\n\\[\n\\begin{pmatrix}\n0.9x_t + 0.6y_t \\\\\n-0.9x_t + 0.6 y_t\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 & 0 \\\\\n0 & -5\n\\end{pmatrix}\n\\begin{pmatrix}\n0.9x_{t - 1} + 0.6y_{t - 1} \\\\\n-0.9x_{t - 1} + 0.6 y_{t - 1}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0.9e_{x,t} + 0.6e_{y,t} \\\\\n-0.9e_{x,t} + 0.6 e_{y,t}\n\\end{pmatrix}\n\\]\nNow, we can solve the difference equation in the simple case. Once we have found the solutions, we can transform it back to the original variables \\(x_t\\) and \\(y_t\\).\nFor the purposes of this course, however, our focus lies on the behavior of the system: Does it converge to a stable path?\nThe answer is in the matrix \\(\\mathbf{A}\\). When we start doing the the recursive substitution that we did in the scalar case, the transformed equations will look like this:\n\\[\n\\begin{align}\n\\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}_{t}} \\\\\n\\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda} (\\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}_{t}}) + \\tilde{\\mathbf{e}_{t}} \\\\\n\\implies \\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda}^2 \\tilde{\\mathbf{z}}_{t - 1} +\\mathbf{\\Lambda} \\tilde{\\mathbf{e}_{t}} + \\tilde{\\mathbf{e}_{t}}\n\\end{align}\n\\]\nYou can continue the substitution just like we did in Exercise 3.2. At this point, you should realize that the system’s behavior depends on the matrix \\(\\mathbf{\\Lambda}\\). If the elements of \\(\\mathbf{\\Lambda|\\) are less than one in absolute value, the system will return to equilibrium after a shock. Otherwise, it will show explosive behavior. This leads us to the following problem: How do we find the values of the diagonal matrix? We will make use of a result from linear algebra.\n\nTheorem 3.2 (Eigenvalues) The eigenvalues of a square matrix \\(\\mathbf{A}\\) are the solutions of the following equation.\n\\[\n\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n\\]"
  },
  {
    "objectID": "03-linear-difference-equations.html#second-order-linear-difference-equations",
    "href": "03-linear-difference-equations.html#second-order-linear-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.3 Second-Order Linear Difference Equations",
    "text": "3.3 Second-Order Linear Difference Equations\nIn a second-order difference equation, the current value \\(y_t\\) depends (directly) on the values up to two periods before it: \\(y_{t - 1}\\) and \\(y_{t - 2}\\).\n\\[\ny_{t} = \\phi_{1}y_{t - 1} + \\phi_{2}y_{t - 2} + e_{t}\n\\]\nAs in the first-order case, we want to investigate the stability property of this process: How does \\(y_{t + j}\\) change in response to variations in the shocks (\\(e_t\\)). It helps to express this equation as a first-order VAR process.\n\\[\n\\begin{align}\nx_{t} & = \\phi_{1}x_{t - 1} + \\phi_{2}x_{t - 2} + e_{t} \\\\\nx_{t - 1} & = x_{t - 1}\n\\end{align}\n\\]\nThe matrix for the VAR process is simple in this case.\n\\[\n\\begin{pmatrix}\nx_t \\\\\nx_{t - 1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\phi_1 & \\phi_2 \\\\\n1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{t - 1} \\\\\nx_{t - 2}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne_{t} \\\\\n0\n\\end{pmatrix}\n\\] We can use Theorem 3.2 to find the eigenvalues of the matrix:\n\\[\n\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n\\] In our case of a second-order difference equation, it is simply\n\\[\n\\det\\begin{pmatrix}\n\\phi_1 - \\lambda & \\phi_2 \\\\\n1 & 0 - \\lambda\n\\end{pmatrix} = 0.\n\\]\n\\[\n\\begin{align}\n(\\phi_1 - \\lambda)(-\\lambda) - \\phi_2 \\cdot 1 = 0 \\\\\n\\lambda^2 - \\lambda \\phi_1 - \\phi_2 = 0\n\\end{align}\n\\]\nThe left-hand side of this equation is called the characteristic polynomial of the difference equation. The whole equation is called the characteristic equation.\nMost of the time, we derive this equation using the lag operator:\n\\[\ny_{t} = \\phi_1 y_{t - 1} + \\phi_2 y_{t - 2} + e_t \\\\\ny_{t} = \\phi_1 L y_{t} + \\phi_2 L^2 y_{t} + e_{t} \\\\\n(1 - \\phi_1 L - \\phi_2 L^2) y_{t} = e_{t}\n\\]\n\nTheorem 3.3 (Solutions of a Quadratic Equation) The solutions of a quadratic equation\n\\[\na \\lambda^2 + b \\lambda + c = 0\n\\]\nare given by:\n\\[\n\\lambda_{1,2} = \\frac{-b \\pm\\sqrt{b^2 - 4ac}}{2a}\n\\]\n\\[\n(\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = 0\n\\]\n\nIn the more general case, you can rely on the Fundamental Theorem of Algebra.\n\nTheorem 3.4 (The Fundamental Theorem of Algebra) Any n-th order polynomial with complex coefficients has exactly n (possibly repeating) complex roots.\n\\[\na_0 + a_1 \\lambda + a_2 \\lambda^2 + \\ldots a_n\\lambda^n = 0\n\\]\n\\[\n(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\\cdot \\ldots\\cdot(\\lambda - \\lambda_n) = 0\n\\]\n\n\nExercise 3.3 (Characteristic roots) Find the characteristic roots of the following difference equation:\n\\[\ny_t = y_{t - 1} + 0.2 y_{t - 2} + e_t\n\\] Use Theorem 3.3 to compute the roots by hand. First, find the characteristic equation using matrix notation, then use the lag polynomial to derive it. Confirm your solution using the polyroot function in R. Is the equation stable?\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory\nTime Series with R. Use R!\nDordrecht Heidelberg: Springer. https://doi.org/10.1007/978-0-387-88698-5.\n\n\nKirchgässner, Gebhard, Jürgen Wolters, and Uwe Hassler. 2013.\nIntroduction to Modern Time Series Analysis. 2. ed. Springer\nTexts in Business and Economics. Berlin Heidelberg:\nSpringer.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. First edition.\nSebastopol, CA: O’Reilly."
  }
]