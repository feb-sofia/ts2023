[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "",
    "text": "General Information"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Schedule",
    "text": "Schedule\n\nTu 12:15-13:45 in room 302\nWe 12:15-13:45 in room 302\nWe 14:15-15:45 in room 308"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Grading",
    "text": "Grading\nThe course consists of two parts: lectures and exercise classes. Your final grade will be the average of the grades of these two parts. To obtain a grade for the exercise classes, you need to complete a homework project at the end of the semester—details to be announced."
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/feb-sofia/ts2023"
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer and accept the default settings.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nOptionally, you can also download and install git. In case of difficulties, these videos may help.\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\nhttps://github.com/feb-sofia/ts2023.git\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete.\n\n\ninstall.packages(c(\"tidyverse\", \"tidyverts\", \"xts\", \"quantmod\", \"urca\"))\n\n\n\n\nInstall packages\n\n\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r"
  },
  {
    "objectID": "index.html#recommended-reading",
    "href": "index.html#recommended-reading",
    "title": "Introduction to Time Series Analysis 2023/2024",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nKirchgässner, Wolters, and Hassler (2013)\nCowpertwait and Metcalfe (2009)\n\n\n\n\n\nCowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory Time Series with R. Use R! Dordrecht Heidelberg: Springer. https://doi.org/10.1007/978-0-387-88698-5.\n\n\nKirchgässner, Gebhard, Jürgen Wolters, and Uwe Hassler. 2013. Introduction to Modern Time Series Analysis. 2. ed. Springer Texts in Business and Economics. Berlin Heidelberg: Springer."
  },
  {
    "objectID": "01-Introduction.html#date-and-time-objects-in-r",
    "href": "01-Introduction.html#date-and-time-objects-in-r",
    "title": "1  Introduction",
    "section": "1.1 Date and Time objects in R",
    "text": "1.1 Date and Time objects in R\nInternally, dates are stored as the number of days since a (arbitrary) reference (origin). The default origin in R is the first of January 1970.\n\norigin &lt;- as.Date(\"1970-01-01\")\njan02.1970 &lt;- as.Date(\"1970-01-02\")\n\norigin\n\n[1] \"1970-01-01\"\n\njan02.1970\n\n[1] \"1970-01-02\"\n\nas.numeric(origin)\n\n[1] 0\n\nas.numeric(jan02.1970)\n\n[1] 1\n\norigin + 5\n\n[1] \"1970-01-06\"\n\norigin + 5:10\n\n[1] \"1970-01-06\" \"1970-01-07\" \"1970-01-08\" \"1970-01-09\" \"1970-01-10\"\n[6] \"1970-01-11\"\n\n\nTime is stored as the number of seconds (and fractions of a second) since the origin: 1970-01-01 00:00:00 UCT. POSIX stands for Portable Operating System Interface.\n\norigin_time &lt;- as.POSIXct(\"1970-01-01 00:00:00\", tz = \"UCT\")\nsome_other_time &lt;- as.POSIXct(\"1970-01-01 00:01:00\", tz = \"UCT\")\n\n# Print the value of origin_time\norigin_time\n\n[1] \"1970-01-01 UTC\"\n\n# Print the value of origin_time\nsome_other_time\n\n[1] \"1970-01-01 00:01:00 UTC\"\n\n# Print the numeric value of origin_time (seconds since the origin)\nas.numeric(origin_time)\n\n[1] 0\n\n# Print the numeric value of some_other_time (seconds since the origin)\nas.numeric(some_other_time)\n\n[1] 60\n\n\n\nclass(origin_time)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nThe package lubridate provides utility functions for working with date and time objects. You can look at this online guide here.\n\n# Sys.time returns the current time\nnow &lt;- Sys.time()\n\n# Print the value of now\nnow\n\n[1] \"2024-01-03 10:05:41 UTC\"\n\n# Day of the month\nday(now)\n\n[1] 3\n\n# Day of the week (depends on the start of week assumption)\nwday(now)\n\n[1] 4\n\nwday(now, week_start = 1)\n\n[1] 3\n\n# Day of the week as string (locale dependent)\nwday(now, label = TRUE)\n\n[1] Wed\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n# Day of the year\nyday(now)\n\n[1] 3\n\n# Week of the year\nweek(now)\n\n[1] 1\n\n# Month of the year\nmonth(now)\n\n[1] 1\n\n# Quarter of the year\nquarter(now)\n\n[1] 1\n\n\n\nExercise 1.1 (Date Objects in R)  \n\nWhat day of the week was the 10-th of June 2019?\nCreate a sequence of 34 consecutive dates starting on the 2-nd of January 2017. What was the year quarter of the last date of this sequence?\n\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "01-Introduction.html#time-series-classes",
    "href": "01-Introduction.html#time-series-classes",
    "title": "1  Introduction",
    "section": "1.2 Time Series Classes",
    "text": "1.2 Time Series Classes\nThere are several classes that are used for storing time series in R. We will mainly focus on tsibble, but some functions we will encounter later in the course return or require ts or xts objects.\n\nts\nxts (see the xts vignette)\ntsibble (see the introduction to tsibble)\n\n\n1.2.1 Creating ts Objects\n\nts(dt$x, start = 1960, frequency = 12)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1960 10.20000 10.40400 10.61208 10.82432 11.04081 11.26162 11.48686 11.71659\n1961 12.93607 13.19479 13.45868 13.72786 14.00241 14.28246 14.56811 14.85947\n          Sep      Oct      Nov      Dec\n1960 11.95093 12.18994 12.43374 12.68242\n1961                                    \n\n\n\nts(dt$x, start = 1960, frequency = 4)\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n1960 10.20000 10.40400 10.61208 10.82432\n1961 11.04081 11.26162 11.48686 11.71659\n1962 11.95093 12.18994 12.43374 12.68242\n1963 12.93607 13.19479 13.45868 13.72786\n1964 14.00241 14.28246 14.56811 14.85947\n\n\n\nts(dt$x, start = 1960, frequency = 1)\n\nTime Series:\nStart = 1960 \nEnd = 1979 \nFrequency = 1 \n [1] 10.20000 10.40400 10.61208 10.82432 11.04081 11.26162 11.48686 11.71659\n [9] 11.95093 12.18994 12.43374 12.68242 12.93607 13.19479 13.45868 13.72786\n[17] 14.00241 14.28246 14.56811 14.85947\n\n\n\n\n1.2.2 Creating xts Objects\nUnlike ts, xts requires an explicit time index in the form of date-like objects:\n\nyearmon (package zoo)\nyearqtr (package zoo)\nDate\nPOSIX\n\n\n# Here, we arbitrarily choose monthly measurements \n# to demonstrate the construction of xts objects\n\n# First, we construct a time index: the months from Feb 1960 to Sep 1961 by using the as.yearmon function. Note that yearmon stores a month as the year plus a fraction\n\nas.yearmon(\"1960-01\")\n\n[1] \"Jan 1960\"\n\nas.numeric(as.yearmon(\"1960-01\"))\n\n[1] 1960\n\nas.yearmon(\"1960-02\")\n\n[1] \"Feb 1960\"\n\nas.numeric(as.yearmon(\"1960-02\"))\n\n[1] 1960.083\n\nas.yearmon(\"1960-03\")\n\n[1] \"Mar 1960\"\n\nas.numeric(as.yearmon(\"1960-03\"))\n\n[1] 1960.167\n\nas.yearmon(\"1960-01\") + 1:20 / 12\n\n [1] \"Feb 1960\" \"Mar 1960\" \"Apr 1960\" \"May 1960\" \"Jun 1960\" \"Jul 1960\"\n [7] \"Aug 1960\" \"Sep 1960\" \"Oct 1960\" \"Nov 1960\" \"Dec 1960\" \"Jan 1961\"\n[13] \"Feb 1961\" \"Mar 1961\" \"Apr 1961\" \"May 1961\" \"Jun 1961\" \"Jul 1961\"\n[19] \"Aug 1961\" \"Sep 1961\"\n\ndt &lt;- dt %&gt;%\n  mutate(\n    # n() counts the number of rows in the table,\n    month = as.yearmon(\"1960-01\") + 1:n() / 12\n  )\n\ndt_xts &lt;- xts(\n  dt, \n  order.by = dt$month\n)\n\ndt_xts\n\n         tidx        x    month\nFeb 1960    1 10.20000 Feb 1960\nMar 1960    2 10.40400 Mar 1960\nApr 1960    3 10.61208 Apr 1960\nMay 1960    4 10.82432 May 1960\nJun 1960    5 11.04081 Jun 1960\nJul 1960    6 11.26162 Jul 1960\nAug 1960    7 11.48686 Aug 1960\nSep 1960    8 11.71659 Sep 1960\nOct 1960    9 11.95093 Oct 1960\nNov 1960   10 12.18994 Nov 1960\nDec 1960   11 12.43374 Dec 1960\nJan 1961   12 12.68242 Jan 1961\nFeb 1961   13 12.93607 Feb 1961\nMar 1961   14 13.19479 Mar 1961\nApr 1961   15 13.45868 Apr 1961\nMay 1961   16 13.72786 May 1961\nJun 1961   17 14.00241 Jun 1961\nJul 1961   18 14.28246 Jul 1961\nAug 1961   19 14.56811 Aug 1961\nSep 1961   20 14.85947 Sep 1961\n\n\nLike xts, tsibble requires an explicit date-like object as an index. You can use the tsibble provided functions yearmonth, yearquarter, etc.\nNote that yearmonth and yearquarter behave differently from as.yearmon and as.yearqtr. yearmonth counts the number of months since the origin. yearquarter tracks the number of quarters since the origin.\n\nyearquarter(\"2021-10\")\n\n&lt;yearquarter[1]&gt;\n[1] \"2021 Q4\"\n# Year starts on: January\n\nas.numeric(yearquarter(\"2021-10\"))\n\n[1] 207\n\n\n\ndt_ts &lt;- dt %&gt;% \n  mutate(\n    x = x, \n    month = yearmonth(\"1960-01\") + 1:n()\n    ) %&gt;%\n  as_tsibble(\n    index = month\n  )\n\ndt_ts\n\n# A tsibble: 20 x 3 [1M]\n    tidx     x    month\n   &lt;int&gt; &lt;dbl&gt;    &lt;mth&gt;\n 1     1  10.2 1960 Feb\n 2     2  10.4 1960 Mar\n 3     3  10.6 1960 Apr\n 4     4  10.8 1960 May\n 5     5  11.0 1960 Jun\n 6     6  11.3 1960 Jul\n 7     7  11.5 1960 Aug\n 8     8  11.7 1960 Sep\n 9     9  12.0 1960 Oct\n10    10  12.2 1960 Nov\n11    11  12.4 1960 Dec\n12    12  12.7 1961 Jan\n13    13  12.9 1961 Feb\n14    14  13.2 1961 Mar\n15    15  13.5 1961 Apr\n16    16  13.7 1961 May\n17    17  14.0 1961 Jun\n18    18  14.3 1961 Jul\n19    19  14.6 1961 Aug\n20    20  14.9 1961 Sep\n\n\nThe package tsbox provides functions that can convert between these classes. You can learn more about the package here.\n\nExercise 1.2 (Tsibbles) The following chunk creates tibble (dt_r) with the integers from 5 to 18 in the column “x”. These are quarterly measurements with the first observation corresponding to 2018Q2. Use mutate to add column holding a time index using the yearquarter function and create a tsibble using the as_tsibble function.\n\ndt_r &lt;- tibble(\n  x = 5:18\n)\n\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "01-Introduction.html#features-of-a-time-series",
    "href": "01-Introduction.html#features-of-a-time-series",
    "title": "1  Introduction",
    "section": "1.3 Features of a Time Series",
    "text": "1.3 Features of a Time Series\n\nTrend\nSeasonal patterns\nCyclical patterns\nNon-systematic fluctuations"
  },
  {
    "objectID": "01-Introduction.html#electricity-production-example",
    "href": "01-Introduction.html#electricity-production-example",
    "title": "1  Introduction",
    "section": "1.4 Electricity Production Example",
    "text": "1.4 Electricity Production Example\nThe dataset electr_r contains monthly values of the electricity available in the internal market (in GWh) in Bulgaria.\n\nelectr_r &lt;- read_csv(\"https://raw.githubusercontent.com/feb-sofia/ts2023/main/data/electricity/bg_internal_consumption.csv\") %&gt;%\n  mutate(\n    electr = as.numeric(electr)\n  ) %&gt;%\n  filter(!is.na(electr)) %&gt;%\n  rename(\n    GWh = electr\n  )\n\n\n# Returns the first rows so that we can get an idea about\n# the contents of the tibble (data table).\n\nhead(electr_r)\n\n# A tibble: 6 × 2\n  month     GWh\n  &lt;chr&gt;   &lt;dbl&gt;\n1 2008-01  3808\n2 2008-02  3281\n3 2008-03  2923\n4 2008-04  2593\n5 2008-05  2449\n6 2008-06  2393\n\n\nThe raw data electr_r contains a column called month that shows the month to which the value in GHw refers. The month column is character (chr) and is not in a format we can use as a time index. We need to parse the text to get a numeric representation of the month. This is what the yearmonth function does (for monthly measurements).\n\nelectr &lt;- electr_r %&gt;%\n  mutate(\n    # Create a new column called ym that holds the numeric \n    # time index\n    ym = yearmonth(month)\n  ) %&gt;%\n  as_tsibble(\n    # This assigns the newly created column ym as the time index\n    index = ym\n  )\n\n\nelectr %&gt;%\n  autoplot(GWh) + \n  labs(\n    x = \"Month\"\n  )\n\n\n\n\nAn advantage of the tsibble objects is that you can use familiar verbs, for example:\n\nmutate: to create new columns or modify existing ones\nselect: to select a subset of columns\nfilter: to select a subset of rows\n\nFor more information, check the documentation here.\n\n# Examples for creating new columns with mutate\n\nelectr1 &lt;- electr %&gt;%\n  mutate(\n    GWh_centered = GWh - mean(GWh),\n    GWh_log = log(GWh),\n  ) %&gt;%\n  select(-GWh)\n\nYou can also use the mean, sd, summary, etc. by selecting the columns from the tsibble using the $ syntax.\n\nelectr$GWh\n\n  [1] 3808 3281 2923 2593 2449 2393 2540 2560 2480 2695 3000 3320 3627 3098 3122\n [16] 2436 2311 2292 2461 2406 2304 2617 2813 3253 3496 3074 3041 2460 2303 2291\n [31] 2401 2495 2212 2721 2545 3219 3541 3176 3174 2502 2297 2363 2499 2480 2353\n [46] 2748 3161 3316 3619 3636 3121 2444 2368 2389 2602 2515 2285 2419 2781 3394\n [61] 3495 3066 3144 2639 2267 2331 2455 2455 2308 2599 2759 3378 3346 2898 2799\n [76] 2560 2431 2370 2501 2488 2375 2665 2966 3273 3491 3048 3104 2579 2306 2307\n [91] 2611 2534 2395 2631 2693 3076 3633 2872 2896 2373 2384 2381 2548 2516 2410\n[106] 2701 2933 3507 3515 2847 2618 2400 2232 2201 2393 2398 2262 2438 2683 2904\n[121] 3103 2802 2785 2037 2154 2211 2317 2361 2235 2414 2737 3216 3327 2806 2649\n[136] 2476 2350 2320 2464 2449 2283 2387 2565\n\n\n\n# Compute the arithmetic average of the values\n# in the GWh column of the data set electr\n\nmean(electr$GWh)\n\n[1] 2706.706\n\n\n\nsummary(electr$GWh)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2037    2393    2560    2707    3020    3808 \n\n\n\nelectr %&gt;%\n  gg_season() + \n  labs(\n    x = \"Month\",\n    colour = \"Year\"\n  )\n\nPlot variable not specified, automatically selected `y = GWh`\n\n\n\n\n\n\nelectr %&gt;%\n  gg_subseries() + \n  labs(\n    x = \"Year\"\n  )\n\nPlot variable not specified, automatically selected `y = GWh`\n\n\n\n\n\nBoth tsibble and xts objects support convenient filtering by the time index. You can check the syntax of both packages here:\n\nxts cheatsheet\ntsibble filter_index\n\n\nelectr %&gt;%\n  filter_index(\"2008-01\"~\"2008-12\")\n\n# A tsibble: 12 x 3 [1M]\n   month     GWh       ym\n   &lt;chr&gt;   &lt;dbl&gt;    &lt;mth&gt;\n 1 2008-01  3808 2008 Jan\n 2 2008-02  3281 2008 Feb\n 3 2008-03  2923 2008 Mar\n 4 2008-04  2593 2008 Apr\n 5 2008-05  2449 2008 May\n 6 2008-06  2393 2008 Jun\n 7 2008-07  2540 2008 Jul\n 8 2008-08  2560 2008 Aug\n 9 2008-09  2480 2008 Sep\n10 2008-10  2695 2008 Oct\n11 2008-11  3000 2008 Nov\n12 2008-12  3320 2008 Dec\n\n\n\nelectr %&gt;%\n  filter_index(\"2008-01\"~\"2008-12\") %&gt;%\n  as_tibble() %&gt;%\n  summarise(\n    mean = mean(GWh)\n  )\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 2837.\n\n\nYou can use index_by and summarise to change the frequency of the data, i.e., to collapse the time series (e.g., from daily to monthly, from monthly to annual, etc.).\n\nelectr %&gt;%\n  index_by(qrt = ~ yearquarter(.)) %&gt;%\n  summarise(\n    GWh = sum(GWh)\n  ) %&gt;%\n  autoplot(GWh)"
  },
  {
    "objectID": "01-Introduction.html#bitcoin-exchange-rate-example",
    "href": "01-Introduction.html#bitcoin-exchange-rate-example",
    "title": "1  Introduction",
    "section": "1.5 Bitcoin Exchange Rate Example",
    "text": "1.5 Bitcoin Exchange Rate Example\n\nExercise 1.3 (Reading and Working with tsibble Objects) The following chunk uses the eurostat package to download quarterly, seasonally unadjusted series in constant 2015 prices for the Bulgarian GDP between 1995 and 2023.\n\ngdp_q &lt;- get_eurostat(\n  \"namq_10_gdp\",\n  filters = list(\n    geo=\"BG\",\n    unit = \"CLV_I15\",\n    na_item = \"B1GQ\",\n    s_adj = \"NSA\"\n  ),\n  cache = FALSE,\n  type = \"code\"\n) %&gt;%\n  filter(!is.na(values)) %&gt;%\n  mutate(\n    time = str_replace(time, \"-\", \" \"),\n    values = as.numeric(values)\n  )\n\n\nCreate a tsibble object to hold the time series. Hint: Use the as_tsibble and yearquarter functions from the tsibble package. Use the code from the introduction\nWhat is the unit of measurement: EUR or BGN? Take a look at the values and make a guess.\nWhat are the dates of the first and the last observations?\nWhat is the average quarterly GDP over the whole period.\nCompute the average GDP by quarter.\nCreate a series of annual GDP measurements by summing the values of the quarterly GDP in each year. Hint: use index_by and summarise.\nIs there a seasonal pattern in the data? Create a seasonal plot using the gg_season function from the tsibble package. Which quarter tends to have the lowest GDP?\nIs there a trend visible in the data?\nCompute the average quarterly GDP between before 2009 and after (and including) 2009.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "02-Statistics-Review.html#expectation",
    "href": "02-Statistics-Review.html#expectation",
    "title": "2  Statistics Review",
    "section": "2.1 Expectation",
    "text": "2.1 Expectation\nThe expected value of a random variable is the average of all possible values that can occurr, weighted by their occurrence probabilities. It is a measure of the location of the distribution.\n\\[\n\\begin{align}\n\\mu_x & = E(X) = \\sum_{x = 0}^{3} x p_X(x) = 0 \\times 0.250 + 1 \\times 0.095 + 2 \\times 0.272 + 3 \\times 0.383 = 1.788 \\\\\n\\end{align}\n\\]\n\nmu_x &lt;- sum(px$x * px$p)\nmu_x\n\n[1] 1.788\n\n\n\nExercise 2.2 (Expected Value) Compute the expected value of \\(Y\\).\n\n\nSolution. \n\n# Type your code here\n\n\nIf you want to predict future values of a random variable, the expected value is your best guess in the sense that it minimises the expected value of the quadratic loss function:\n\\[\nE[(X - \\hat{x})^2]\n\\]\nLet us construct an example. You need to predict the result of \\(X\\) and you think that the best prediction is \\(\\bar{x} = 1\\). When the game runs it will produce four possible values: 0, 1, 2, and 3. The error that you will make is:\n\\[\nL(x) = (x - 1)^2 =\n\\begin{cases}\n   (0 - 1)^2 = 1 & \\text{x = 0}\\\\\n   (1 - 1)^2 = 0 & \\text{x = 1}\\\\\n   (2 - 1)^2 = 1 & \\text{x = 2}\\\\\n   (3 - 1)^2 = 4 & \\text{x = 3}\n\\end{cases}\n\\]\n\nExercise 2.3 (Expected Quadratic Loss) Compute the expected quadratic loss for a prediction \\(\\bar{x} = 1.5\\).\n\n\nSolution. \n\n## Type your code here\n\n# px %&gt;%\n#   mutate(\n#     loss = ?\n#   )"
  },
  {
    "objectID": "02-Statistics-Review.html#variance",
    "href": "02-Statistics-Review.html#variance",
    "title": "2  Statistics Review",
    "section": "2.2 Variance",
    "text": "2.2 Variance\nThe variance of a random variable measures how different the possible values that can occur are. Values that occur more often (have higher probability) under \\(p_X\\) receive a higher weight. Values that occur less frequently under \\(p_X\\) are given a lower weight in the sum.\n\\[\n\\begin{align}\nVar(X) & = \\sum_{x = 0}^{3} (x - \\mu_x)^2 \\times p_X(x) \\\\\n       & = (0 - 1.788)^2 \\times 0.250 + (1 - 1.788)^2 \\times 0.095 + (2 - 1.788)^2\\times 0.272 + (3 - 1.788)^2 \\times 0.383 \\\\\n       & = (-1.788)^2 \\times 0.250 + (-0.788)^2 \\times 0.095 + (0.212)^2\\times 0.272 + (1.212)^2 \\times 0.383 \\\\\n       & = 3.196 \\times 0.250 + 0.620^2 \\times 0.095 + 0.044 \\times 0.272 + 1.468 \\times 0.383 \\\\\n       & \\approx 1.433\n\\end{align}\n\\tag{2.1}\\]\n\n(px$x - mu_x)\n\n[1] -1.788 -0.788  0.212  1.212\n\n(px$x - mu_x)^2\n\n[1] 3.196944 0.620944 0.044944 1.468944\n\npx$p * (px$x - mu_x)^2\n\n[1] 0.79923600 0.05898968 0.01222477 0.56260555\n\nsum(px$p * (px$x - mu_x)^2)\n\n[1] 1.433056\n\n\nYou can see from Equation 2.1 that it is the expected value of the squared diviations from the expected value.\n\\[\nVar(X) = E(X - E(X))^2\n\\]\n\nDefinition 2.1 (Variance) The variance of a random variable (distribution) is a summary of the distribution and describes its spread: how different are the values that this distribution will generate.\n\\[\nVar(X) = E[(X - E(X))^2] = E(X^2) - E(X)^2\n\\]\n\n\nExercise 2.4 (Variance) Compute the Variance of \\(Y\\).\n\n\nSolution. \n\n# Type your code here\n\n\n\nTheorem 2.1 (Properties of the Expectation) Let \\(X\\) be a random variable with expected value \\(E(X)\\), let \\(Y\\) be a random variable with expected value \\(E(Y)\\), and let \\(a\\) be a fixed constant (\\(a \\in \\mathbb{R}\\)). The following properties are true:\n\\[\n\\begin{align}\nE(a) & = a \\\\\nE(aX) & = aE(X) \\\\\nE(X + Y) & = E(X) + E(Y)\n\\end{align}\n\\]\nFurthermore, if \\(X\\) and \\(Y\\) are uncorrelated, then the expected value of the product of the two random variables equals the product of their expected values:\n\\[\nE(XY) = E(X)E(Y)\n\\]\n\n\nTheorem 2.2 (Properties of the Variance) Let \\(X\\) be a random variable with expected value \\(E(X)\\), let \\(Y\\) be a random variable with expected value \\(E(Y)\\), and let \\(a\\) be a fixed constant (\\(a \\in \\mathbb{R}\\)). The following properties are true:\n\\[\nVar(X) = E(X^2) - E(X)^2\n\\]\n\\[\n\\begin{align}\nVar(a) & = 0 \\\\\nVar(aX) & = a^2 Var(X)\n\\end{align}\n\\] Furthermore, if \\(X\\) and \\(Y\\) are uncorrelated, then the variance of their sum equals the sum of their variances:\n\\[\nVar(X + Y) = Var(X) + Var(Y)\n\\]\n\n\nExercise 2.5 (Expected value and variance) Use the distributions of \\(X\\) and \\(Y\\) from Table 2.1 and Table 2.2 to compute the expected value and the variance of\n\\[\n2X + 3Y + 1.\n\\]\nAssume that \\(X\\) and \\(Y\\) are independent.\n\n\nSolution. \\[\nE(2X + 3Y + 1) = \\\\\nVar(2X + 3Y + 1) =\n\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#joint-distribution",
    "href": "02-Statistics-Review.html#joint-distribution",
    "title": "2  Statistics Review",
    "section": "2.3 Joint Distribution",
    "text": "2.3 Joint Distribution\nIt is as summary of the joint distribution of \\(X\\) and \\(Y\\). The joint probability mass function tells you the probability of the simultaneous occurrence of \\(x\\) and \\(y\\). For example, you can ask it the question: what is the probability of \\(x = 2\\) and \\(y = 3\\).\nFor two discrete variables, it is convenient to present the joint distribution as a table with cell entries holding the probabilities. The joint distribution is given in the tibble pxy in a long format.\n\npxy %&gt;%\n  knitr::kable()\n\n\n\n\nx\ny\np\n\n\n\n\n0\n2\n0.241\n\n\n0\n3\n0.009\n\n\n1\n2\n0.089\n\n\n1\n3\n0.006\n\n\n2\n2\n0.229\n\n\n2\n3\n0.043\n\n\n3\n2\n0.201\n\n\n3\n3\n0.182\n\n\n\n\n\nSometimes it is more convenient to see this distribution in a wide format:\n\npxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix=\"y=\"\n    ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.3: Joint distribution of \\(X\\) and \\(Y\\).\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.241\n0.009\n\n\n1\n0.089\n0.006\n\n\n2\n0.229\n0.043\n\n\n3\n0.201\n0.182\n\n\n\n\n\n\n\\[\np_{XY}(x=2, y=3) = 0.043\n\\]\nThe joint probability distribution function must sum (integrate) to one over all possible pairs of \\(x\\) and \\(y\\).\n\\[\n\\sum_{x = 0}^{3}\\sum_{y = 2}^{3} p_{XY}(x, y) = 1\n\\]\n\nsum(pxy$p)\n\n[1] 1\n\n\nIn the example until now we have summarized the marginal distributions of \\(X\\) and \\(Y\\) but we have said nothing about their joint distribution. Usually the joint distribution is determined by the subject matter at hand, but for the sake of example we will look at two joint distributions so that we can get an idea how they work.\nFirst we will construct a special joint distribution under the assumption of independence. Intuitively, two random variables are independent, if the outcome of one of the variables does not influence the probability distribution of the other. Imagine that you hold two lottery tickets: one from a lottery in Germany and another from a lottery in Bulgaria. It would be safe to assume that the realized winnings from the German lottery will not affect the odds to win from the Bulgarian ticket.\nNow let us consider a case of dependent random variables. Let \\(X\\) be the level of a river (at some measurement point) at time \\(t\\) and \\(Y\\) be the level of the same river five minutes later. It woule be safe to assume that if the level of the river was high at \\(t\\) this would affect the distribution of the level of the river at \\(t\\) plus five minutes."
  },
  {
    "objectID": "02-Statistics-Review.html#marginal-distributions",
    "href": "02-Statistics-Review.html#marginal-distributions",
    "title": "2  Statistics Review",
    "section": "2.4 Marginal Distributions",
    "text": "2.4 Marginal Distributions\nThe marginal distribution of \\(X\\) is obtained by summing the joint distribution of \\(X\\) and \\(Y\\) over all possible values of \\(Y\\).\n\\[\np_X(x) = \\sum_{y=2}^{3}p_{XY}(x, y)\n\\]"
  },
  {
    "objectID": "02-Statistics-Review.html#conditional-distributions",
    "href": "02-Statistics-Review.html#conditional-distributions",
    "title": "2  Statistics Review",
    "section": "2.5 Conditional Distributions",
    "text": "2.5 Conditional Distributions\n\npxy_w &lt;- pxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  ) %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  )\n\npxy_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.4: Conditional distributions of \\(Y\\) given \\(X\\)\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.964\n0.036\n0.250\n\n\n1\n0.937\n0.063\n0.095\n\n\n2\n0.842\n0.158\n0.272\n\n\n3\n0.525\n0.475\n0.383\n\n\n\n\n\n\nLooking at the conditional distributions of \\(Y\\) given \\(X\\) in Table 2.4, you should notice that these are not the same for each value of \\(X\\). For example, \\(Y=2\\) is much more likely when \\(X = 0\\) compared to \\(X = 3\\)."
  },
  {
    "objectID": "02-Statistics-Review.html#joint-distribution-under-independence",
    "href": "02-Statistics-Review.html#joint-distribution-under-independence",
    "title": "2  Statistics Review",
    "section": "2.6 Joint Distribution under Independence",
    "text": "2.6 Joint Distribution under Independence\nLets construct the joint distribution \\(p_{XY}(x, y)\\) that assigns a probability to the points \\((x, y)\\), assuming that \\(X\\) and \\(Y\\) are independent.\nFor independent random variables the joint probability of occurrence is simply the product of the marginal distributions.\n\\[\np_{XY}(x, y) = p_X(x)p_Y(y)\n\\]\n\npxy_ind &lt;- expand_grid(\n  px %&gt;% rename(p_x = p), \n  py %&gt;% rename(p_y = p)\n)\npxy_ind &lt;- pxy_ind %&gt;%\n  mutate(\n    p = p_x * p_y\n  )\n\n\npxy_ind_w &lt;- pxy_ind %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  )\n\npxy_ind_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.5: Joint distribution of \\(X\\) and \\(Y\\) under independence.\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.190\n0.060\n\n\n1\n0.072\n0.023\n\n\n2\n0.207\n0.065\n\n\n3\n0.291\n0.092\n\n\n\n\n\n\nLet’s look at the conditional distributions of \\(Y\\) given \\(X\\). These answer the questions of the type: if \\(X\\) turns out to be \\(0\\), what are the probabilities for \\(Y = 2\\) and \\(Y = 3\\).\nTo get the conditional distributions of Y for each possible value of \\(X\\) we divide the cells of the joint distribution table by the marginal probabilities of each \\(x\\).\n\\[\np_{Y|X}(x, y) = \\frac{p_{XY}(x, y)}{p_X(x)}\n\\]\n\npxy_ind_w %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 2.6: Conditional distributions of \\(Y\\). Independence case.\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.76\n0.24\n0.250\n\n\n1\n0.76\n0.24\n0.095\n\n\n2\n0.76\n0.24\n0.272\n\n\n3\n0.76\n0.24\n0.383\n\n\n\n\n\n\nWhat you should see in Table 2.6 is that the conditional distributions of \\(Y\\) are the same for every possible value of \\(X\\). This is of course a consequence of the way we constructed this joint distribution in the first place: namely, we assumed that \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "02-Statistics-Review.html#conditional-expectation",
    "href": "02-Statistics-Review.html#conditional-expectation",
    "title": "2  Statistics Review",
    "section": "2.7 Conditional Expectation",
    "text": "2.7 Conditional Expectation\nWe have seen how we derived the conditional distributions of \\(Y\\) given \\(X\\) in the previous section. Now we can ask the question: what is the expected value of \\(Y\\) given that \\(X\\) has already turned out to be 0 (for example). We can take the conditional distribution of \\(Y\\) given \\(X = 0\\) and compute the expected value of this distribution.\nFor the joint distribution under independence:\n\\[\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.76 + 3 \\times 0.24 = 2.24\n\\]\n\n2 * 0.76 + 3 * 0.24\n\n[1] 2.24\n\n\nFor the joint distribution in Table 2.4 the conditional expectation of \\(Y\\) given \\(X = 0\\) is\n\\[\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.964 + 3 \\times 0.036 = 2.036\n\\]\n\n2 * 0.964 + 3 * 0.036\n\n[1] 2.036\n\n\nLet us write the conditional expectation of \\(Y\\) for each possible value of \\(X\\) for the dependent joint distribution case.\n\\[\nE(Y | X = x) = \\begin{cases}\n  2.036 & \\text{for } x = 0 \\\\\n  2.060 & \\text{for } x = 1 \\\\\n  2.158 & \\text{for } x = 2 \\\\\n  2.475 & \\text{for } x = 3\n\\end{cases}\n\\]\n\npxy %&gt;%\n  group_by(x) %&gt;%\n  summarise(\n    y = y,\n    p_y_x = p / sum(p)\n  ) %&gt;%\n  summarise(\n    E_Y_given_X = sum(y * p_y_x) \n  ) %&gt;%\n  knitr::kable(digits = 3)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'x'. You can override using the `.groups`\nargument.\n\n\n\n\nTable 2.7: Conditional expectation of \\(Y\\) for each possible value of \\(X\\).\n\n\nx\nE_Y_given_X\n\n\n\n\n0\n2.036\n\n\n1\n2.063\n\n\n2\n2.158\n\n\n3\n2.475\n\n\n\n\n\n\nAn important thing to see here is that the conditional expectation is different for each value of \\(X\\). As the value of \\(X\\) is uncertain (it is a random variable), the conditional expectation of \\(Y\\) given \\(X\\) is also a random variable. Its distribution is given by the possible values and the probabilities of occurrence of \\(X\\) (the marginal distribution of \\(X\\)).\n\nExercise 2.6 Calculate the expected value of \\(Y\\) given \\(X\\) for every possible value of \\(X\\) in the case joint distribution under independence.\n\n\nExample 2.2 (Sampling from the Joint Distribution)  \n\nsample_joint &lt;- pxy %&gt;%\n  slice_sample(n = 1000, weight_by = p, replace = TRUE)\n\nhead(sample_joint)\n\n# A tibble: 6 × 3\n      x     y     p\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     3     2 0.201\n2     0     2 0.241\n3     3     2 0.201\n4     3     2 0.201\n5     3     3 0.182\n6     2     2 0.229\n\n\n\nsample_joint %&gt;%\n  group_by(x, y) %&gt;%\n  summarise(\n    p = first(p),\n    n = n(),\n    f = n / nrow(sample_joint)\n  )\n\n`summarise()` has grouped output by 'x'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 5\n# Groups:   x [4]\n      x     y     p     n     f\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1     0     2 0.241   243 0.243\n2     0     3 0.009    10 0.01 \n3     1     2 0.089    85 0.085\n4     1     3 0.006     5 0.005\n5     2     2 0.229   226 0.226\n6     2     3 0.043    43 0.043\n7     3     2 0.201   199 0.199\n8     3     3 0.182   189 0.189"
  },
  {
    "objectID": "02-Statistics-Review.html#covariance",
    "href": "02-Statistics-Review.html#covariance",
    "title": "2  Statistics Review",
    "section": "2.8 Covariance",
    "text": "2.8 Covariance\nThe covariance measures the (linear) dependency between two random variables.\n\nDefinition 2.2 (Covariance) The covariance of two random variables \\(X\\) and \\(Y\\) is given by\n\\[\nCov(X, Y) = E[(X - E(X))(Y - E(Y))]\n\\] Alternatively, it can be computed using the decomposition formula:\n\\[\nCov(X, Y) = E(XY) - E(X)E(Y)\n\\]\n\nIn the analysis of time series we will often encounter situations where the expected value of one of the random variables is zero. As can be seen from the decomposition formula, in that case the covariance reduces to\n\\[\nCov(X, Y) = E(XY).\n\\]\nClosely related to the covariance is the correlation between \\(X\\) and \\(Y\\).\n\nDefinition 2.3 (Correlation) \\[\n\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}\n\\] It is easy to show that the correlation is bounded between -1 and 1.\n\\[\n-1 \\leq \\rho(X, Y) \\leq 1\n\\]\n\n\nExercise 2.7 (Correlation) Let X be a random variable with, and \\(Y = a + bX\\). Show that the correlation between \\(X\\) and \\(Y\\) equals one or minus one depending on the sign of \\(b\\). For simplicity, assume that \\(E(X) = 0\\).\n\n\nTheorem 2.3 (Properties of the Covariance) Let \\(X\\) and \\(Y\\) be random variables and let \\(a, b \\in \\mathbb{R}\\) be fixed constants.\n\\[\nVar(aX + bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X, Y)\n\\]\n\n\nExercise 2.8 (Covariance) Compute the covariance of \\(X\\) and \\(Y\\) under the joint distributions given in Table 2.5 and Table 2.3. Use the pxy and pxy_ind tables for these calculations.\n\n\nSolution. \n\n# Type your code here\n\n\n\nExercise 2.9 (Variance of Correlated Variables) Compute the variance of \\(2X - Y\\) using the same distributions as in Exercise 2.8."
  },
  {
    "objectID": "02-Statistics-Review.html#empirical-estimation-of-moments",
    "href": "02-Statistics-Review.html#empirical-estimation-of-moments",
    "title": "2  Statistics Review",
    "section": "2.9 Empirical Estimation of Moments",
    "text": "2.9 Empirical Estimation of Moments\nThroughout this course we will focus on three summaries of the stochastic processes: the mean (expected value, level), the variance (fluctuation), and the covariances/correlations (dependency) between lags of the random process.\nIn the the previous statistics-related courses we had a sample of (uncorrelated) observations \\(x_1, x_2,\\ldots, x_n\\). We assumed that these are samples (realizations) from some normal distribution \\(N(\\mu, \\sigma^2)\\) with unknown mean and variance. We used the observed values to learn something about this distribution. We estimated its mean with\n\\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\\]\nand its variance with:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n  -1}\\sum_{i = 1}^{n}(x_i - \\hat{\\mu})^2\n\\tag{2.2}\\]\nIn the case of time series, however, we have only one observation for each time point.\n\nn &lt;- 10\nB &lt;- 5\nset.seed(21)\n\ndt_rw &lt;- tibble(\n  r = rep(1:B, n),\n  e = rnorm(n = B*n)\n) %&gt;%\n  group_by(r) %&gt;%\n  mutate(\n    t = 1:n,\n    x = cumsum(e),\n    is_observed = (r == 1),\n  )\n\ndt_rw %&gt;%\n  ggplot(\n    aes(\n      x = t,\n      y = x, \n      group = r, \n      alpha = factor(is_observed))\n    ) +\n  geom_line() + \n  geom_point() + \n  labs(\n    alpha = \"Observed\",\n    y = expression(x[t])\n  ) + \n  scale_x_continuous(breaks = 1:n)\n\n\n\n\nIn order to be able to apply our usual technique for moments estimation we have to assume that the random process is ergodic, meaning that we can estimate its statistical properties (mean, variance, correlations) from the its time series (observed values). We will talk more extensively about (weak) stationarity, which is a necessary (though not sufficient) condition for ergodicity. For (weakly) stationary time series the mean, variance and covariances do not change over time.\n\nDefinition 2.4 (Autocovariances and Autocorrelations) \\[\n\\begin{align}\n\\mu & = E(X_1) = E(X_2) = \\ldots = E(X_T) \\\\\n\\gamma(0) & = Var(X_1) = Var(X_2) = \\ldots = Var(X_T) \\\\\n\\gamma(1) & = Cov(X_1, X_2) = Cov(X_2, X_3) = \\ldots = Cov(X_{t - 1}, X_{t}) = Cov(X_{T - 1}, X_{T}) \\\\\n\\gamma(k) & = Cov(X_{t}, X_{t + k}) \\\\\n\\rho(k) & = \\frac{\\gamma(k)}{\\gamma(0)}\n\\end{align}\n\\]\n\nFor a random process \\(X_1, X_2,\\ldots, X_T\\) and an observed time series \\(x_1, x_2, \\ldots, x_T\\), we will estimate the mean with the arithmetic average of the observed values.\n\\[\n\\hat{\\mu} = \\frac{1}{T}\\sum_{t = 1}^{T} x_{t}\n\\]\nWe will estimate the variance of a stationary process with the empirical variance:\n\\[\n\\hat{\\gamma}(0) = \\frac{1}{T}\\sum_{t = 1}^{T}(x_t - \\bar{x})^2\n\\tag{2.3}\\]\nAnd the covariances with the empirical (auto-)covariances:\n\\[\n\\hat{\\gamma}(k) = \\frac{1}{T}\\sum_{t = 1}^{T - k} (x_{t} - \\hat{\\mu})(x_{t + k} - \\hat{\\mu})\n\\tag{2.4}\\]\nand empirical (auto-)correlations:\n\\[\n\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)}\n\\] To see the meaning of the autocovariances, let us create two artificial time series and plot their lags. The time series will be generated from two processes. The first one will be called the purely random or white noise process.\n\\[\ne_t \\sim N(0, 1) \\text{ for every } t \\\\\nE(e_t) = 0 \\\\\nVar(e_t) = 1 \\\\\nCov(e_t, e_{t + k}) = 0, k \\neq 0\n\\] The second one is be called a random walk.\n\\[\ny_t = \\sum_{i = 1}^{t} e_i\n\\]\n\nset.seed(432)\n\n# Create 20 observations from two time series. rnorm draws random numbers\n# from a normal distribution with zero expected value (mean) and standard\n# deviation (sd) equal to 1. We call this distribution the standard normal\n# distribution. In the mutate step we create the first two lags of the series\n\ndt_sim &lt;- tibble(\n  x = rnorm(n = 20, mean = 0, sd = 1),\n  y = cumsum(x)\n) %&gt;%\n  mutate(\n    x_l1 = lag(x, n = 1),\n    y_l1 = lag(y, n = 1)\n  )\n\nmean_x &lt;- mean(dt_sim$x)\nmean_x\n\n[1] -0.08259397\n\nmean_y &lt;- mean(dt_sim$y)\nmean_y\n\n[1] 0.8611093\n\n\n\ndt_sim %&gt;%\n  ggplot(aes(x = x, y = x_l1)) +\n  geom_point() + \n  geom_vline(xintercept=mean_x, lty=2, alpha = 0.5) + \n  geom_hline(yintercept=mean_x, lty=2, alpha = 0.5) + \n  labs(\n    x = \"x\",\n    y = \"lag(x, 1)\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nLooking at the plot, think about the signs of the cross-products that go into the formula for the covariance:\n\\[\n(x_{t + 1} - \\hat{\\mu})(x_{t} - \\hat{\\mu})\n\\]\nWhere do you find positive and negative cross-products in the plot?\n\ndt_sim %&gt;%\n  ggplot(aes(x = y_l1, y = y)) +\n  geom_point() + \n  geom_vline(xintercept=mean_y, lty=2, alpha = 0.5) + \n  geom_hline(yintercept=mean_y, lty=2, alpha = 0.5) + \n  labs(\n    x = \"lag(y, 1)\",\n    y = \"y\"\n  )\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe following code computes the covariance\n\n# Compute the covariance\ncov(dt_sim$x, dt_sim$x_l1, use = \"complete.obs\")\n\n[1] -0.1063495\n\n# COmpute the correlation\ncor(dt_sim$x, dt_sim$x_l1, use = \"complete.obs\")\n\n[1] -0.1821236\n\n\nNote that cov computes the covariance according to formula Equation 2.2."
  },
  {
    "objectID": "02-Statistics-Review.html#exercises",
    "href": "02-Statistics-Review.html#exercises",
    "title": "2  Statistics Review",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\nThe following code downloads\n\nbitcoin &lt;- getSymbols(\n  \"BTC-USD\", \n  from=\"2015-01-01\",\n  to=\"2023-01-01\",\n  src=\"yahoo\", \n  periodicity=\"daily\", \n  env = NULL)\n\nnames(bitcoin) &lt;- c(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adjusted\")\n\nbc &lt;- bitcoin %&gt;%\n  ts_tsibble() %&gt;%\n  pivot_wider(\n    names_from = id,\n    values_from = value\n  )\n\n\n# Plots the closing exchange rate against the time index\n\nbc %&gt;%\n  autoplot(Close) + \n  labs(\n    # Set the labels for the x and y axis\n    x = \"Date\",\n    y = \"USD/BTC\"\n  )\n\n\n\n\n\nCompute the average exchange rate (Close).\n\n\n# Type your code here\n\n\nCompute the empirical variance using the var function\n\n\n# Type your code here\n\n\nConstruct a two new variables that contain the first and the second lags of Close. Leave only Close and its lags in the resulting tsibble. Hint: Use the mutate and lag functions and select the columns using select.\n\n\n# Type your code here\n# Select the code below and press Ctrl-Shift-c in order to remove the comments\n\n\nCreate a scatterplot for Close and its first lag. Hint: Use the code generating the scatterplot in the simulation above and adapt it.\n\n\n# Type your code here\n\n\nCompute the covariance between Close and its first lag. Compute the autocorrelation coefficient (empirical autocorrelation) between Close and its first lag. Hint: Look into the documentation of cov and cor to find out how to handle missing values.\n\n\n# Type your code here\n\n\nUse the gg_lag function to create scatterplots for the between the time series and its first six lags. What patterns can you see in these scatterplots?\n\n\nbc %&gt;%\n  gg_lag(\n    Close, \n    lags = 1:6, \n    geom=\"point\", \n    size = 0.5\n  )\n\n\n\n\n\nUse the acf function to plot the autocorrelation function of the Close time series.\n\n\n# Type your code here"
  },
  {
    "objectID": "03-linear-difference-equations.html#first-order-difference-equations",
    "href": "03-linear-difference-equations.html#first-order-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.1 First Order Difference Equations",
    "text": "3.1 First Order Difference Equations\nDifference equations describe the dynamic behavior of a sequence by relating the current value of a variable to its previous values.\nLet us start with a simple difference equation defined by:\n\\[\ny_{t} = \\phi y_{t - 1} + e_t\n\\]\nFor example, imagine that it describes the value of your bank deposit. Suppose you receive an interest of 2 percent annually. In that case, the value of your account this year (\\(y_t\\)) is equal to the value of the account the previous year plus the interest earned (\\(\\phi = 1 + 0.02\\)). You can think about \\(e_t\\) as withdrawals or additional deposits to your account.\nWe want to achieve two goals. The first goal is to solve this equation by expressing \\(y_t\\) as a function of some initial value \\(y_0\\), the time index \\(t\\), the parameter \\(\\phi\\) and the sequence of \\(e_t\\).\nIt helps if we write the equation for a couple of periods.\n\\[\n\\begin{align}\n1 & \\quad y_1 = \\phi y_{0} + e_{1} \\\\\n2 & \\quad y_2 = \\phi y_{1} + e_{2} \\\\\n3 & \\quad y_3 = \\phi y_{2} + e_{3} \\\\\n& \\vdots \\\\\nt & \\quad y_{t} = \\phi y_{t - 1} + e_{t} \\\\\nt + 1 &  \\quad y_{t + 1} = \\phi y_{t} + e_{t + 1}\\\\\nt + 2 & \\quad y_{t + 2} = \\phi y_{t + 1} + e_{t + 2}\n\\end{align}\n\\]\nTry substituting the first equation into the second, then the second into the third equation.\n\n\n\n\n\n\nClick here to see the solution\n\n\n\n\n\n\\[\ny_3 = \\phi^3 y_0 + \\phi^2 e_{1} + \\phi e_{2} + \\phi^0e_{3}\n\\]\n\n\n\nYou can generalize this for \\(y_t\\):\n\\[\ny_{t + j} = \\phi^{j + 1} y_{t - 1} + \\phi^{j}e_{t} + \\phi^{j - 1} e_{t + 1} + \\ldots \\phi^1 e_{t + j - 1} + \\phi^0 e_{t + j}\n\\] We can write this more compactly as:\n\\[\ny_{t + j} = \\phi^{j + 1} y_{t - 1} + \\sum_{k = 0}^{j} \\phi^{k} e_{t + j - k}\n\\]\nYou should check that the above formula is correct. Let’s write it down for \\(j = 2\\).\n\n\n\n\n\n\nClick here to see the result\n\n\n\n\n\n\\[\n\\begin{align}\ny_{t + 2} = \\phi^{2 + 1}y_{t - 1} & + \\phi^0 e_{t + 2 - 0} \\\\\n& + \\phi^{1} e_{t + 2 - 1} \\\\\n& + \\phi^{2} e_{t + 2 - 2} \\\\\n\\end{align}\n\\]\n\n\n\nNow that we know how to solve the equation, we can analyze the behavior of the dynamic system that it describes. It turns out that this behavior is determined by the parameter \\(\\phi\\).\nLet’s investigate the effect of a unit change in \\(e_t\\) on \\(y_{t + j}\\), assuming that all other values of \\(e\\) remain unchanged.\n\\[\n\\frac{\\partial y_{t + j}}{\\partial e_{t}} = \\phi^j\n\\] Note that the effect on \\(y_{t}\\) only depends on the distance in time between the shock \\(e_t\\) and the response \\(y_{t + j}\\), not on the time index itself. This is a property of linear difference equations.\nNow, obviously, a unit shock in time \\(t\\) will have a different effect on \\(y_{t + j}\\) depending on the value of \\(\\phi\\).\nAnother effect that we would like to analyze is the effect of a permanent unit change in the autonomous process \\(e_t\\). What will happen to \\(y_t\\) if all \\(e_t\\) increase by one unit?\n\n\n\n\n\n\nClick here to see the result\n\n\n\n\n\n\\[\ny_{t + j} = \\phi^{j + 1}y_{t - 1} + \\sum_{k = 0}^{j}\\phi^k e_{t + j - k}\n\\]\n\\[\n\\begin{align}\ny^{*}_{t + j} & = \\phi^{j + 1}y_{t - 1} & + \\sum_{k = 0}^{j}\\phi^k (e_{t + j - k} + 1) \\\\\n              & = \\phi^{j + 1}y_{t - 1} & + \\sum_{k = 0}^{j}\\phi^k e_{t + j - k} + \\sum_{k = 0}^{j}\\phi^k\n\\end{align}\n\\]\nFrom this, it should be easy to see that\n\\[\ny^{*}_{t + j} - y_{t + j} = \\sum_{k = 0}^{j}\\phi^k.\n\\]\n\n\n\nThe effect of a permanent unit change in \\(e_t\\) is thus a power series in the parameter \\(\\phi\\). At this point, it is helpful to remember a theorem from school about the limit of this series.\n\nTheorem 3.1 (Convergence of a Geometric Series) \\[\nS_{n} = 1 + \\phi + \\phi^2 + \\phi^3 + \\ldots + \\phi^n\n\\]\nAssuming that \\(\\phi \\neq 1\\), this series can be expressed more compactly as\n\\[\nS_{n} = \\frac{1 - \\phi^{n + 1}}{1 - \\phi}\n\\]\nTo see this, compute the difference between \\(S_n\\) and \\(\\phi S_n\\) and rearrange.\nIf \\(|\\phi| &lt; 1 \\iff -1 &lt; \\phi &lt; 1\\) the series has a limit for \\(n \\to \\infty\\):\n\\[\n\\lim_{n \\to \\infty} S_{n} = \\frac{1}{1 - \\phi}\n\\]\n\n\nExercise 3.1 (Geometric Series) To see how this series behaves, give it a try in R.\n\nphi &lt;- 0.5\n# This will give you a vector of the first 11 elements\n\ndt_gseries &lt;- tibble(\n  n = 0:10,\n  el = phi^n,\n# The geometric series is simply the cumulative sum of these elements\n  series = cumsum(el)\n)\n\ndt_gseries %&gt;%\n  ggplot(aes(x = n, y = series)) + \n  geom_line()\n\n\n\n\n\nLet us check\n\nset.seed(321)\n\n# Set the parameter phi\nphi &lt;- 0.5\n# Set the initial value (y_{t - 1}) in the above notation\ny_init &lt;- 1\n# Set the number of periods j\nB &lt;- 1\n\ndt &lt;- tibble(\n  j = 0:B,\n  # Generate random values for e\n  e = rnorm(n = B + 1),\n  # Add one to the first row of e\n  e_trans = e + c(1, rep(0, B)),\n  # Add one to all values of e\n  e_perm = e + 1,\n  # Compute the powers of phi used to weight e/e_trans/e_perm\n  phi_powers = phi^(B - j)\n)\n\ndt %&gt;%\n  summarise(\n    # The following simply implements the formula for y\n    y = phi^(B + 1) * y_init + sum(e * phi_powers),\n    y_trans = phi^(B + 1) * y_init + sum(e_trans * phi_powers),\n    y_perm = phi^(B + 1) * y_init + sum(e_perm * phi_powers),\n    y_diff_trans = y - y_trans,\n    y_diff_perm = y - y_perm\n  ) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\n\ny\ny_trans\ny_perm\ny_diff_trans\ny_diff_perm\n\n\n\n\n0.39\n0.89\n1.89\n-0.5\n-1.5\n\n\n\n\n\n\nExercise 3.2 Given a first order difference equation with \\(\\phi = 0.8\\), find the value of the process in \\(t = 3\\), knowing that it started with \\(y_0 = 5\\). Assume that \\(e_t = 0\\) for all \\(t\\).\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "03-linear-difference-equations.html#first-order-vector-difference-equations",
    "href": "03-linear-difference-equations.html#first-order-vector-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.2 First-Order Vector Difference Equations",
    "text": "3.2 First-Order Vector Difference Equations\nIn this course, we will focus exclusively on univariate time series analysis. Still, for the sake of motivating the study of higher-order linear difference equations, it is convenient to introduce vector linear difference equations. Let’s start with two variables, \\(x_t\\) and \\(y_t\\).\n\\[\n\\begin{align}\nx_{t} & = \\phi_{x,1}x_{t - 1} + \\phi_{y, 1}y_{t - 1} + e_{x,t} \\\\\ny_{t} & = \\phi_{x,2}x_{t - 1} + \\phi_{y, 2}y_{t - 1} + e_{y,t}\n\\end{align}\n\\]\nIt is convenient to write this equation in matrix form.\n\\[\n\\underbrace{\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}}_{\\mathbf{z}_t}\n= \\underbrace{\\begin{pmatrix}\n\\phi_{x, 1} & \\phi_{y, 1} \\\\\n\\phi_{x, 2} & \\phi_{y, 2}\n\\end{pmatrix} }_{\\mathbf{A}}\n\\underbrace{\\begin{pmatrix}\nx_{t - 1} \\\\\ny_{t - 1}\n\\end{pmatrix}}_{\\mathbf{z}_{t - 1}} +\n\\underbrace{\\begin{pmatrix}\ne_{x,t} \\\\\ne_{y,t}\n\\end{pmatrix}}_{\\mathbf{e}_t}\n\\]\n\\[\n\\mathbf{z}_{t} = \\mathbf{A} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t}\n\\]\nIf \\(\\mathbf{A}\\) is a diagonal matrix, then we already know the solution because we can solve each equation separately.\n\\[\n\\underbrace{\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}}_{\\mathbf{z}_t}\n= \\underbrace{\\begin{pmatrix}\n\\phi_{x,1} & 0 \\\\\n0 & \\phi_{y, 2}\n\\end{pmatrix} }_{\\mathbf{A}}\n\\underbrace{\\begin{pmatrix}\nx_{t - 1} \\\\\ny_{t - 1}\n\\end{pmatrix}}_{\\mathbf{z}_{t - 1}} +\n\\underbrace{\\begin{pmatrix}\ne_{x,t} \\\\\ne_{y,t}\n\\end{pmatrix}}_{\\mathbf{e}_t}\n\\]\nHowever, \\(\\mathbf{A}\\) depends on the subject matter at hand, and we need to be able to handle the general case (not diagonal). Linear algebra provides an elegant solution to our problem. The eigendecomposition of the matrix \\(\\mathbf{A}\\) allows us to transform the system of equations into a form where we can apply the simple solution. After solving the equations, we can back-transform to the original variables.\n\\[\n\\mathbf{A} = \\mathbf{V}\n\\begin{pmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{pmatrix}\n\\mathbf{V}^{-1}\n\\]\nUsing this decomposition, we can transform the complicated problem to a simple one with only diagonal entries in the matrix before \\(\\mathbf{z}_{t - 1}\\).\n\\[\n\\begin{align}\n\\mathbf{z}_{t} & = \\mathbf{A} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t} \\\\\n\\mathbf{z}_{t} & = \\mathbf{V} \\mathbf{\\Lambda}\\mathbf{V^{-1}} \\mathbf{z}_{t - 1} + \\mathbf{e}_{t} \\\\\n\\mathbf{V}^{-1} \\mathbf{z}_{t} & = \\mathbf{V}^{-1} \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\mathbf{z}_{t - 1} + \\mathbf{V}^{-1}\\mathbf{e}_{t} \\\\\n\\mathbf{V}^{-1} \\mathbf{z}_{t} & = \\mathbf{\\Lambda}\\mathbf{V}^{-1} \\mathbf{z}_{t - 1} + \\mathbf{V}^{-1}\\mathbf{e}_{t} \\\\\n\\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}_{t}}\n\\end{align}\n\\]\nLets see an example.\n\nB &lt;- matrix(c(1, 9, 4, 1), ncol = 2)\nB\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    9    1\n\n\n\ned &lt;- eigen(B)\ned\n\neigen() decomposition\n$values\n[1]  7 -5\n\n$vectors\n          [,1]       [,2]\n[1,] 0.5547002 -0.5547002\n[2,] 0.8320503  0.8320503\n\n\n\nsolve(ed$vectors)\n\n           [,1]      [,2]\n[1,]  0.9013878 0.6009252\n[2,] -0.9013878 0.6009252\n\n\n\ned$vectors %*% diag(ed$values) %*% solve(ed$vectors)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    9    1\n\n\nIn this example the diagonal matrix \\(\\mathbf{\\Lambda}\\) is equal to\n\\[\n\\mathbf{\\Lambda} = \\begin{pmatrix}\n7 & 0 \\\\\n0 & -5\n\\end{pmatrix}\n\\] The inverse of the eigenvectors matrix is\n\\[\n\\mathbf{V}^{-1} = \\begin{pmatrix}\n0.9 & 0.6 \\\\\n-0.9 & 0.6\n\\end{pmatrix}\n\\]\nThe transformed vectors \\(\\tilde{\\mathbf{z}_{t}}\\) will look like:\n\\[\n\\tilde{\\mathbf{z}_{t}} = \\mathbf{V}^{-1}\\mathbf{z}_t = \\begin{pmatrix}\n0.9 & 0.6 \\\\\n-0.9 & 0.6\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{t} \\\\\ny_{t}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.9x_t + 0.6y_t \\\\\n-0.9x_t + 0.6 y_t\n\\end{pmatrix}\n\\]\nThe whole system looks like:\n\\[\n\\begin{pmatrix}\n0.9x_t + 0.6y_t \\\\\n-0.9x_t + 0.6 y_t\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n7 & 0 \\\\\n0 & -5\n\\end{pmatrix}\n\\begin{pmatrix}\n0.9x_{t - 1} + 0.6y_{t - 1} \\\\\n-0.9x_{t - 1} + 0.6 y_{t - 1}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0.9e_{x,t} + 0.6e_{y,t} \\\\\n-0.9e_{x,t} + 0.6 e_{y,t}\n\\end{pmatrix}\n\\]\nNow, we can solve the difference equation in the simple case. Once we have found the solutions, we can transform them back to the original variables \\(x_t\\) and \\(y_t\\).\nFor the purposes of this course, however, our focus lies on the behavior of the system: Does it converge to a stable path?\nThe answer is in the matrix \\(\\mathbf{A}\\). When we start doing the the recursive substitution that we did in the scalar case, the transformed equations will look like this:\n\\[\n\\begin{align}\n\\tilde{\\mathbf{z}}_{t} & = \\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}}_{t} \\\\\n\\tilde{\\mathbf{z}}_{t + 1} & = \\mathbf{\\Lambda} (\\mathbf{\\Lambda} \\tilde{\\mathbf{z}}_{t - 1} + \\tilde{\\mathbf{e}}_{t}) + \\tilde{\\mathbf{e}}_{t + 1} \\\\\n\\implies \\tilde{\\mathbf{z}}_{t + 1} & = \\mathbf{\\Lambda}^2 \\tilde{\\mathbf{z}}_{t - 1} +\\mathbf{\\Lambda} \\tilde{\\mathbf{e}}_{t} + \\tilde{\\mathbf{e}}_{t + 1}\\\\\n\\tilde{\\mathbf{z}}_{t + j} & = \\mathbf{\\Lambda}^{j + 1} \\tilde{\\mathbf{z}}_{t - 1} + \\sum_{k = 0}^{j} \\mathbf{\\Lambda}^{k} \\tilde{\\mathbf{e}}_{t + j - k}\n\\end{align}\n\\]\nYou can continue the substitution just like we did in Exercise 3.2. At this point, you should realize that the system’s behavior depends on the matrix \\(\\mathbf{\\Lambda}\\). If the elements of \\(\\mathbf{\\Lambda|\\) are less than one in absolute value, the system will return to equilibrium after a shock. Otherwise, it will show explosive behavior. This leads us to the following problem: How do we find the values of the diagonal matrix? We will make use of a result from linear algebra.\n\nTheorem 3.2 (Eigenvalues) The eigenvalues of a square matrix \\(\\mathbf{A}\\) are the solutions of the following equation.\n\\[\n\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n\\]"
  },
  {
    "objectID": "03-linear-difference-equations.html#second-order-linear-difference-equations",
    "href": "03-linear-difference-equations.html#second-order-linear-difference-equations",
    "title": "3  Linear Difference Equations",
    "section": "3.3 Second-Order Linear Difference Equations",
    "text": "3.3 Second-Order Linear Difference Equations\nIn a second-order difference equation, the current value \\(y_t\\) depends (directly) on the values up to two periods before it: \\(y_{t - 1}\\) and \\(y_{t - 2}\\).\n\\[\ny_{t} = \\phi_{1}y_{t - 1} + \\phi_{2}y_{t - 2} + e_{t}\n\\]\nAs in the first-order case, we want to investigate the stability property of this process: How does \\(y_{t + j}\\) change in response to variations in the shocks (\\(e_t\\)). It helps to express this equation as a first-order VAR process.\n\\[\n\\begin{align}\nx_{t} & = \\phi_{1}x_{t - 1} + \\phi_{2}x_{t - 2} + e_{t} \\\\\nx_{t - 1} & = x_{t - 1}\n\\end{align}\n\\]\nThe matrix for the VAR process is simple in this case.\n\\[\n\\begin{pmatrix}\nx_t \\\\\nx_{t - 1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\phi_1 & \\phi_2 \\\\\n1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{t - 1} \\\\\nx_{t - 2}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\ne_{t} \\\\\n0\n\\end{pmatrix}\n\\] We can use Theorem 3.2 to find the eigenvalues of the matrix:\n\\[\n\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\n\\] In our case of a second-order difference equation, it is simply\n\\[\n\\det\\begin{pmatrix}\n\\phi_1 - \\lambda & \\phi_2 \\\\\n1 & 0 - \\lambda\n\\end{pmatrix} = 0.\n\\]\n\\[\n\\begin{align}\n(\\phi_1 - \\lambda)(-\\lambda) - \\phi_2 \\cdot 1 = 0 \\\\\n\\lambda^2 - \\lambda \\phi_1 - \\phi_2 = 0\n\\end{align}\n\\]\nThe left-hand side of this equation is called the characteristic polynomial of the difference equation. The whole equation is called the characteristic equation.\nMost of the time, we derive this equation using the lag operator:\n\nDefinition 3.1 (Lag Operator) The lag operator returns the previous (shifted) value of a series.\n\\[\n\\begin{align}\n& L y_{t} = y_{t - 1}\\\\\n& L^2 y_{t} = L(Ly_t) = L(y_{t - 1}) = y_{t - 2} \\\\\n& L^k y_{t} = y_{t - k}\n\\end{align}\n\\]\n\nWe can rewrite the difference equation in terms of a polynomial of the lag operator.\n\\[\n\\begin{align}\ny_{t} = \\phi_1 y_{t - 1} + \\phi_2 y_{t - 2} + e_t \\\\\ny_{t} = \\phi_1 L y_{t} + \\phi_2 L^2 y_{t} + e_{t} \\\\\ny_{t}(\\underbrace{(1 - \\phi_1 L - \\phi_2 L^2)}_{\\text{Lag Polynomial}}) = e_{t}\n\\end{align}\n\\]\nThe lag polynomial and the characteristic polynomial are related.\n\\[\n\\begin{align}\n& 1 L^0 - \\phi_1 L^1 - \\phi_2 L^2 \\\\\n& 1 \\lambda^{2 - 0} - \\phi_1 \\lambda^{2 - 1} - \\phi_2 \\lambda^{2 - 2} \\\\\n& 1 \\lambda^{2} - \\phi_1 \\lambda^{1} - \\phi_2 \\lambda^{0} \\\\\n& \\lambda^{2} - \\phi_1 \\lambda - \\phi_2 \\\\\n\\end{align}\n\\]\n\nTheorem 3.3 (Solutions of a Quadratic Equation) The solutions of a quadratic equation\n\\[\na \\lambda^2 + b \\lambda + c = 0\n\\]\nare given by:\n\\[\n\\lambda_{1,2} = \\frac{-b \\pm\\sqrt{b^2 - 4ac}}{2a}\n\\]\n\\[\n(\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = 0\n\\]\n\nIn the more general case, you can rely on the Fundamental Theorem of Algebra.\n\nTheorem 3.4 (The Fundamental Theorem of Algebra) Any n-th order polynomial with complex coefficients has exactly n (possibly repeating) complex roots.\n\\[\na_0 + a_1 \\lambda + a_2 \\lambda^2 + \\ldots a_n\\lambda^n = 0\n\\]\n\\[\n(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\\cdot \\ldots\\cdot(\\lambda - \\lambda_n) = 0\n\\]\n\n\nExercise 3.3 (Characteristic roots) Find the characteristic roots of the following difference equation:\n\\[\ny_t = y_{t - 1} + 0.2 y_{t - 2} + e_t\n\\] Use Theorem 3.3 to compute the roots by hand. First, find the characteristic equation using matrix notation, then use the lag polynomial to derive it. Confirm your solution using the polyroot function in R. Is the equation stable?\n\n\nSolution. \n\n# Type your code here"
  },
  {
    "objectID": "04-AR.html#the-purely-random-process",
    "href": "04-AR.html#the-purely-random-process",
    "title": "4  Autoregressive Processes",
    "section": "4.1 The Purely Random Process",
    "text": "4.1 The Purely Random Process\nIn the last class we discussed linear difference equations and their stability. Here we will introduce randomness into the difference equation by assuming that the autonomous terms \\(e_t\\) are uncorrelated and follow a distribution with zero mean and constant variance (not depending on the time index).\n\nDefinition 4.1 (The Purely Random Process) A stochastic process \\(e_t\\) with the follwing three properties is called a purely random or a white noise process.\n\\[\n\\begin{align}\n& E(e_t) = 0 \\text{ for all } t \\\\\n& Var(e_t) = \\sigma^2, \\sigma^2 \\in \\mathbb{R} \\\\\n& Cov(e_{t}, e_{t + k}) = 0, k \\neq 0\n\\end{align}\n\\]\n\nFrom the definitions of variance (Definition 2.1) and covariance (Definition 2.2) you can see that the covariance between two time points is simply the expected value of the cross product of the terms, because \\(E(e_t) = 0\\) for all \\(t\\).\n\\[\nCov(e_t, e_{t + k}) = E(e_{t}e_{t + k}) = 0\n\\]\nWhen \\(k = 0\\), this expression reduces to\n\\[\nCov(e_t, e_{t + 0}) = E(e_t e_t) = E(e_t^2) = \\sigma^2\n\\] How does such a process look like? It is convenient to generate random values in order to visualize it.\n\nn &lt;- 100\n\ndt &lt;- tibble(\n  t = 1:n,\n  e = rnorm(\n    # The number of values to be drawn at random\n    n = n, \n    # The expected value of the distribution\n    mean = 0,\n    # The standard deviation of the distribution\n    sd = 2\n    )\n)\n\ndt %&gt;%\n  ggplot(aes(x = t, y = e)) + \n  geom_line()\n\n\n\n\nIn order to see how the two parameters of the normal distribution affect the shape of the resulting series, the following code chunk shows four series:\n\\[\n\\begin{align}\ne^{(1)}_t & \\sim N(\\mu = 0, \\sigma^2 = 1) \\\\\ne^{(2)}_t & \\sim N(\\mu = 10, \\sigma^2 = 1) \\\\\ne^{(3)}_t & \\sim N(\\mu = 0, \\sigma^2 = 3^2) \\\\\ne^{(4)}_t & \\sim N(\\mu = 10, \\sigma^2 = 3^2)\n\\end{align}\n\\]\n\n## For illustration only\n\nn &lt;- 100\n\nsim_data &lt;- expand_grid(\n  mu = c(0, 10),\n  sigma = c(1, 3),\n  t = 1:n\n) %&gt;%\n  mutate(\n    y = rnorm(n(), mean = mu, sd = sigma),\n    sigma_lab = paste0(\"sigma = \", sigma),\n    mu_lab = paste0(\"mu = \", mu)\n  )\n\nsim_data %&gt;%\n  ggplot(aes(x = t, y = y, color = mu_lab)) +\n  geom_point(size = 1 / 2) +\n  geom_line() +\n  facet_wrap(~sigma_lab) +\n  labs(\n    x = \"t\",\n    y = expression(y[t]),\n    color = \"Expected value\"\n  )\n\n\n\n\n\nsim_data %&gt;%\n  group_by(mu_lab, sigma_lab) %&gt;%\n  summarise(\n    Average = mean(y),\n    StdDev = sd(y)\n  )\n\n`summarise()` has grouped output by 'mu_lab'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   mu_lab [2]\n  mu_lab  sigma_lab Average StdDev\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 mu = 0  sigma = 1 -0.0389  1.04 \n2 mu = 0  sigma = 3 -0.187   2.66 \n3 mu = 10 sigma = 1 10.1     0.982\n4 mu = 10 sigma = 3 10.3     3.27 \n\n\nNow that we have seen some realizations of a couple of purely random processes, let’s look at the empirical summaries of the time series.\n\nmean(dt$e)\n\n[1] -0.03722768\n\n\n\nsd(dt$e)\n\n[1] 2.032668\n\n\nLet’s also compute is (empirical) auto-correlation coefficients applying the formula\n\\[\n\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)} \\\\\n\\hat{\\rho}(k) = \\frac{\\sum_{t = 1}^{T - k}(y_t - \\hat{\\mu})(y_{t + k} - \\hat{\\mu})}{\\sum_{t = 1}^{T}(y_t - \\hat{\\mu})}\n\\]\nwhere \\(\\hat{\\mu}\\) is the sample average of the series.\n\ndt &lt;- dt %&gt;%\n  mutate(\n    e_l1 = lag(e, n = 1),\n    e_l2 = lag(e, n = 2),\n    e_l3 = lag(e, n = 3)\n  )\n\n\ncor(dt$e, dt$e_l1, use = \"complete.obs\")\n\n[1] 0.0960488\n\ncor(dt$e, dt$e_l2, use = \"complete.obs\")\n\n[1] -0.02261752\n\ncor(dt$e, dt$e_l3, use = \"complete.obs\")\n\n[1] -0.06311071\n\n\nYou can obtain these auto-correlation coefficients using the acf function:\n\nacf(dt$e, plot = FALSE)\n\n\nAutocorrelations of series 'dt$e', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000  0.096 -0.023 -0.062 -0.156 -0.052  0.145  0.091 -0.005 -0.003 -0.230 \n    11     12     13     14     15     16     17     18     19     20 \n-0.103  0.008  0.116 -0.021 -0.104 -0.100 -0.097  0.045 -0.007  0.153 \n\n\nand you can visualize the auto-correlation coefficients\n\nacf(dt$e)\n\n\n\n\nEven though the time series was generated from a process without zero correlations, the empirical auto-correlation coefficients will generally be non-zero. In order to assess whether the observed correlations are compatible with zero theoretical correlations, you can use the confidence interval\n\\[\n0 \\pm \\frac{2}{\\sqrt{T}}\n\\] that is shown as the two dashed horizontal lines in the plot.\nThe Box-Ljung test is a statistical test for the hypothesis\n\\[\n\\rho_{1} = \\rho_2 = \\rho_{k} = 0\n\\]\nand is based on the test statistic:\n\\[\nQ = T(T + 2)\\sum_{j = 1}^{k}\\frac{\\hat{\\rho}^2(j)}{T - j}\n\\] Under the null hypothesis (i.e., if we assume the null hypothesis to be true) it follows a \\(\\Chi^2\\) distribution with \\(k\\) degrees of freedom.\n\nBox.test(dt$e, lag = 3)\n\n\n    Box-Pierce test\n\ndata:  dt$e\nX-squared = 1.3581, df = 3, p-value = 0.7154"
  },
  {
    "objectID": "04-AR.html#ar1",
    "href": "04-AR.html#ar1",
    "title": "4  Autoregressive Processes",
    "section": "4.2 AR(1)",
    "text": "4.2 AR(1)\nThe autoregressive (AR) process of first order is defined by\n\\[\ny_{t} = \\phi_0 + \\phi_1 y_{t - 1} + e_t\n\\]\nwhere \\(\\phi_0, \\phi_1 \\in \\mathbb{R}\\) are fixed (non-random) constants, and \\(e_t\\) is a purely random process.\nWe would like to derive the statistical properties of the process (expected value, variance and correlations) from the model definitions.\nLet’s give it a try. To derive it we use the model definition, the properties of the expected value operator from Theorem 2.1, and the properties of the purely random process in Definition 4.1. The expected value of the process is then\n\\[\n\\begin{align}\nE(y_t) & = E(\\phi_0 + \\phi_1 y_{t - 1} + e_t) \\\\\n       & = E(\\phi_0) + E(\\phi_1 y_{t - 1}) + E(e_t) \\\\\n       & = \\phi_0 + \\phi_1 E(y_{t - 1}) + 0 \\\\\n       & = \\phi_0 + \\phi_1 E(y_{t - 1})\n\\end{align}\n\\] Now we may seem to have hit a dead end, because knowing the expected value of \\(E(y_t)\\) requires the knowledge of \\(E(y_{t - 1})\\). However, if we assume that the expected value does not depend on the time index, so that\n\\[\nE(y_t) = E(y_{t - 1}) = \\mu\n\\]\nthen the equation is very easy to solve:\n\\[\n\\begin{align}\n\\underbrace{E(y_{t})}_{\\mu} & = \\phi_0 + \\phi_1 \\underbrace{E(y_{t - 1})}_{\\mu} \\\\\n\\mu & = \\phi_0 + \\phi_1 \\mu \\\\\n(1 - \\phi_1)\\mu & = \\phi_0 \\\\\n\\mu & = \\frac{\\phi_0}{1 - \\phi_1}\n\\end{align}\n\\]\nIn other words\n\\[\n\\mu = E(y_{t}) = E(y_{t - 1}) = \\frac{\\phi_0}{1 - \\phi_1}\n\\tag{4.1}\\]\nThis derivation begs the question: are we allowed to assume that the expected value does not change over time? The answer is yes, if the process is mean-stationary. This is just another way to say that the expected value does not change but we can derive a condition when this is the case.\nTo see how the stability condition for the first order difference equation relates to the expected value, consider the solution to the equation that we derived last time. The only new thing here is the constant term \\(\\phi_0\\).\n\\[\ny_{t} = \\phi_1^{t} y_{0} + \\frac{1 - \\phi_1^{t}}{1 - \\phi_1} \\phi_0 + \\sum_{k = 0}^{t - 1} \\phi_1^{k} u_{t - k}\n\\]\n\nExercise 4.1 (Variance of a Stationary AR(1)) What is the variance of an AR(1) process given by:\n\\[\ny_{t} = \\phi_0 + \\phi_1 y_{t - 1} + e_{t}, e_{t} \\sim N(0, \\sigma^2)\n\\] Use the properties of the variance in Theorem 2.2 and the properties of the purely random process to derive it.\n\\[\nVar(y_{t}) = ?\n\\]\n\n\nExercise 4.2 (Autocorrelations of a AR(1) process) Express the first and second order auto-correlations of the AR(1) process in terms of its coefficients \\(\\phi_0\\) and \\(\\phi_1\\). Without a loss of generality, assume that the process has a zero expected value.\n\nThe derivation of the autocorrelations of MA(q) processes was relatively easy. For AR(1) processes you can derive the Yule-Walker equations that link the model coefficients and the autocovariances (and therefore the autocorrelations).\nWithout a loss of generality, assume that the constant in the process is zero: \\(\\phi_0 = 0\\). This implies that the expected value of the process is zero (see Equation 4.1):\n\\[\n\\phi_0 = 0 \\implies \\mu = E(y_t) = E(y_{t - k}) = 0 \\text{ for all } k\n\\]\nFor a variable with zero expected value the variance and covariances are simply the expected value of the the cross-products (see Theorem 2.2).\n\\[\n\\begin{align}\n& Cov(y_t, y_{t - k}) = E(y_t y_{t - k}) \\\\\n& Var(y_{t}) = Cov(y_t, y_{t - 0}) = E(y_t y_{t}) = E(y_t^2) \\\\\n\\end{align}\n\\] Now it is very easy to compute the covariance: just take the process definition for a AR(1) and multiply both sides of the equation by \\(y_{t - k}\\)\n\\[\n\\begin{align}\ny_{t} & = \\phi_1 y_{t - 1} + e_t \\\\\ny_{t - k}y_{t} & = \\phi_1 y_{t - k} y_{t -1} + y_{t - k} e_t\n\\end{align}\n\\] Now take the expectation on both sides of the equation and apply the properties of the expected value from Theorem 2.1.\n\\[\n\\begin{align}\nE(y_{t - k}y_{t}) & = E(\\phi_1 y_{t - k} y_{t -1} + y_{t - k} e_t) \\\\\n& = \\phi_1 E(y_{t - k}y_{t - 1}) + E(y_{t - k}e_t)\n\\end{align}\n\\] Now there is only one crucial thing that you must realize. The last expected value in the equation is a product of \\(y_{t - k}\\) and \\(e_{t}\\). It is zero for all \\(k &gt; 0\\) but it equals \\(\\sigma^2\\) for \\(k=0\\). Why? Let’s write it down for \\(k = 0\\).\n\\[\n\\begin{align}\nE(y_{t - 0}e_t) = E(y_t e_t) & = E\\left((\\phi_1 y_{t - 1} + e_t) e_t \\right) \\\\\n& = E(\\phi_1 y_{t - 1} e_t + e_t^2) \\\\\n& = \\phi_1 E(y_{t - 1} e_t) + E(e_t^2) \\\\\n& = \\phi_1 E(y_{t - 1} e_t) + Var(e_t) \\\\\n& = \\phi_1 E(y_{t - 1} e_t) + \\sigma^2 \\\\\n\\end{align}\n\\tag{4.2}\\]\nNow keep in mind that \\(y_{t - 1}\\) depends only on values of \\(e_t\\) that occur before \\(t\\), so it must have zero covariance with \\(e_{t}\\)\n\\[\ny_{t - 1} = \\phi_1 y_{t - 2} + e_{t - 1}\n\\]\nand therefore\n\\[\nE(y_{t - 1}e_{t}) = 0\n\\]\nThe same also holds for the process variables before \\(t - 1\\) such as \\(y_{t - k}\\).\nTherefore we can express Equation 4.2 as\n\\[\nE(y_{t - k}y_{t}) = \\phi_1 E(y_{t - k}y_{t - 1}) + \\unicode{x1D7D9}_{k = 0}\\sigma^2\n\\] To simplify the notation a little bit, we will use the symbols for the autocovariances and autocorrelations defined in Definition 2.4.\n\\[\n\\begin{align}\n\\gamma(k) = \\phi_1 \\gamma(k - 1) + \\unicode{x1D7D9}_{k = 0}\\sigma^2\n\\end{align}\n\\] Now we can write the equation for several values of \\(k\\):\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1 \\gamma(0 - 1) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1 \\gamma(1 - 1) + 0 \\\\\n& k = 2: \\gamma(2) = \\phi_1 \\gamma(2 - 1) + 0 \\\\\n& k = 3: \\gamma(3) = \\phi_1 \\gamma(3 - 1) + 0 \\\\\n\\end{align}\n\\] Simplifying a bit:\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1 \\gamma(- 1) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1 \\gamma(0) \\\\\n& k = 2: \\gamma(2) = \\phi_1 \\gamma(1) \\\\\n& k = 3: \\gamma(3) = \\phi_1 \\gamma(2) \\\\\n\\end{align}\n\\] Now you need one final bit to complete the Yule-Walker equations. The autocovariances are symmetric, i.e. \\(\\gamma(-k) = \\gamma(k)\\). So finally:\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1 \\gamma(1) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1 \\gamma(0) \\\\\n& k = 2: \\gamma(2) = \\phi_1 \\gamma(1) \\\\\n& k = 3: \\gamma(3) = \\phi_1 \\gamma(2) \\\\\n\\end{align}\n\\] You can obtain the autocorrelations easily by dividing each equation by the variance of the process \\(\\gamma(0)\\)\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1 \\gamma(1) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1 \\gamma(0) \\\\\n& k = 2: \\gamma(2) = \\phi_1 \\gamma(1) \\\\\n& k = 3: \\gamma(3) = \\phi_1 \\gamma(2) \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1 \\gamma(1) + \\sigma^2 \\quad / \\gamma(0)\\\\\n& k = 1: \\gamma(1) = \\phi_1 \\gamma(0)  \\quad / \\gamma(0) \\\\\n& k = 2: \\gamma(2) = \\phi_1 \\gamma(1)  \\quad / \\gamma(0)\\\\\n& k = 3: \\gamma(3) = \\phi_1 \\gamma(2)  \\quad / \\gamma(0)\\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& k = 0: \\rho(0) = \\frac{\\gamma(0)}{\\gamma(0)} = 1\\\\\n& k = 1: \\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\phi_1\\\\\n& k = 2: \\rho(2) = \\frac{\\gamma(2)}{\\gamma(0)} = \\phi_1 \\frac{\\gamma(1)}{\\gamma(0)} \\\\\n& k = 3: \\rho(3) = \\frac{\\gamma(3)}{\\gamma(0)} = \\phi_1 \\frac{\\gamma(2)}{\\gamma(0)} \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& k = 0: \\rho(0) = 1\\\\\n& k = 1: \\rho(1) = \\phi_1\\\\\n& k = 2: \\rho(2) = \\phi_1 \\rho(1) \\\\\n& k = 3: \\rho(3) = \\phi_1  \\rho(2) \\\\\n\\end{align}\n\\] To compute \\(\\rho(3)\\) for example, you need to substitute for \\(\\rho(2)\\):\n\\[\n\\rho(3) = \\phi_1 (\\phi_1 \\rho(1)) = \\phi_1 (\\phi_1 (\\phi_1)) = \\phi_1^3\n\\] More generally you obtain\n\\[\n\\rho(k) = \\phi_1^{k}\n\\] Lets see how this works for AR(3)\n\\[\ny_t = \\phi_1 y_{t - 1} + \\phi_2y_{t - 2} + \\phi_3 y_{t - 3} + e_t\n\\]\nMultiply the whole equation by \\(y_{t - k}\\) and take expectations on both sides\n\\[\ny_{t - k}y_t = \\phi_1 y_{t - k}y_{t - 1} + \\phi_2 y_{t - k} y_{t - 2} + \\phi_3 y_{t - k} y_{t - 3} + y_{t - k}e_t\n\\]\n\\[\nE(y_{t - k}y_t) = \\phi_1 E(y_{t - k}y_{t - 1}) + \\phi_2 E(y_{t - k} y_{t - 2}) + \\phi_3 E(y_{t - k} y_{t - 3}) + E(y_{t - k}e_t)\n\\]\nFollowing the same reasoning as for the AR(1) process above and using the shorthand notation for the autocovariances:\n\\[\n\\gamma(k) = \\phi_1\\gamma(k - 1) + \\phi_2 \\gamma(k - 2) + \\phi_3 \\gamma(k - 3) + \\unicode{x1D7D9}_{k = 0}\\sigma^2\n\\] Write it down for \\(k = 0, 1, 2\\)\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1\\gamma(-1) + \\phi_2\\gamma(-2) + \\phi_3 \\gamma(-3) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1\\gamma(1 - 1) + \\phi_2\\gamma(1 -2) + \\phi_3 \\gamma(1 -3) \\\\\n& k = 2: \\gamma(2) = \\phi_1\\gamma(2 - 1) + \\phi_2\\gamma(2 -2) + \\phi_3 \\gamma(2 -3) \\\\\n\\end{align}\n\\]\nSimplify and use the symmetry of autocovariances\n\\[\n\\begin{align}\n& k = 0: \\gamma(0) = \\phi_1\\gamma(1) + \\phi_2\\gamma(2) + \\phi_3 \\gamma(3) + \\sigma^2 \\\\\n& k = 1: \\gamma(1) = \\phi_1\\gamma(0) + \\phi_2\\gamma(1) + \\phi_3 \\gamma(2) \\\\\n& k = 2: \\gamma(2) = \\phi_1\\gamma(1) + \\phi_2\\gamma(0) + \\phi_3 \\gamma(1) \\\\\n\\end{align}\n\\]\n:::{exr-yule-walker-lecture}"
  },
  {
    "objectID": "04-AR.html#characteristic-roots-and-the-yule-walker-equations",
    "href": "04-AR.html#characteristic-roots-and-the-yule-walker-equations",
    "title": "4  Autoregressive Processes",
    "section": "4.3 Characteristic Roots and the Yule-Walker equations",
    "text": "4.3 Characteristic Roots and the Yule-Walker equations\nA solution to one of the problems from the lecture slides. You are given the characteristic roots of a third order autoregressive process. The task in this exercise is to use these roots to find the first and second order autocorrelations of this process. So, the roots of the characteristic equation are: 0.8, 0.2, -0.5. Roots of what? Remember that the lag operator shifts a series (Definition 3.1):\n\\[\nLy_{t} = y_{t - 1}\\\\\nLy_{t - 1} = y_{t - 2}\\\\\nL(L_{t}) = y_{t - 2}\\\\\nL^2y_{t} = y_{t - 2}\\\\\nL^3y_{t} = y_{t - 3}\n\\] So you can express the AR(3) process in terms of its lag-polynomial:\n\\[\ny_{t} = \\phi_0 + \\phi_{1}y_{t - 1} + \\phi_2 y_{t - 2} + \\phi_3 y_{t - 3} + e_t, \\quad e_t \\sim N(0, \\sigma^2)\n\\] \\[\n\\begin{align}\ny_{t} & = \\phi_0 + \\phi_1 Ly_{t} + \\phi_2 L^2y_{t} + \\phi_3 L^3 y_{t} + e_t\\\\\ny_{t}(\\underbrace{L^0 - \\phi_1 L - \\phi_2 L^2 - \\phi_3 L^3}_{\\text{lag polynomial}}) & = \\phi_0 + e_t\n\\end{align}\n\\] The characteristic equation is derived from the lag-polynomial as follows:\n\\[\n\\begin{align}\n\\lambda^{3 - 0} - \\phi_1\\lambda^{3 - 1} - \\phi_2 \\lambda^{3 - 2} - \\phi_3 \\lambda ^ {3 - 3} & = 0\\\\\n\\lambda^{3} - \\phi_1\\lambda^{2} - \\phi_2 \\lambda - \\phi_3 & = 0\n\\end{align}\n\\] The roots of the characteristic polynomial determine whether the process is stationary or not. If the roots of this equation are less than 1 in absolute value then the process is stationary.\nIf we know that the roots of this equation are\n\\[\n\\lambda_1^* = 0.8, \\lambda_2^* = 0.2, \\lambda_3^* = -0.5\n\\] \\[\n\\lambda^{3} - \\phi_1\\lambda^{2} - \\phi_2 \\lambda - \\phi_3 = (\\lambda - \\lambda_{1}^*)(\\lambda - \\lambda_2^*)(\\lambda - \\lambda_3^*) = 0\n\\] \\[\n\\begin{align}\n(\\lambda - \\lambda_{1}^*)(\\lambda - \\lambda_2^*)(\\lambda - \\lambda_3^*) = \\\\\n(\\lambda - 0.8)(\\lambda - 0.2)(\\lambda + 0.5) = \\\\\n\\left[\\lambda^2 - \\lambda + 0.16\\right](\\lambda + 0.5) = \\\\\n\\lambda^3 - \\lambda^2 + 0.16\\lambda + 0.5\\lambda^2 - 0.5\\lambda + 0.08\n\\end{align}\n\\]\n\\[\n\\lambda^{3} - \\phi_1\\lambda^{2} - \\phi_2 \\lambda - \\phi_3\\\\\n\\lambda^3 -0.5\\lambda^2 -0.34\\lambda + 0.08\n\\] So we have now recovered the process coefficients from the roots of the characteristic equation.\n\\[\n\\begin{align}\n\\phi_1 = 0.5\\\\\n\\phi_2 = 0.34\\\\\n\\phi_3 = -0.08\n\\end{align}\n\\] \\[\n\\begin{align}\ny_{t} & = \\phi_0 + \\phi_{1}y_{t - 1} + \\phi_2 y_{t - 2} + \\phi_3 y_{t - 3} + e_t\\\\\ny_{t} & = \\phi_0 + 0.5y_{t - 1} + 0.34y_{t - 2} - 0.08 y_{t - 3} + e_t\n\\end{align}\n\\] Now we can calculate\nFirst order autocorrelation: \\[\n\\rho(y_{t}, y_{t - 1}) = \\rho_1 = \\frac{Cov(y_{t}, y_{t - 1})}{Var(y_{t})}\n\\] The Yule-Walker equations connect the autocorrelations (and autocovariances) and the model coefficients.\n\\[\n\\begin{align}\n\\rho_1 & = \\phi_1 \\rho_0 + \\phi_2\\rho_1 + \\ldots + \\phi_R\\rho_{R - 1}\\\\\n\\rho_2  & = \\phi_1 \\rho_1 + \\phi_2 + \\ldots + \\phi_R \\rho_{R - 2} \\\\\n\\vdots\\\\\n\\rho_{R - 1}  & = \\phi_1 \\rho_{R - 1} + \\phi_2\\rho_{R - 2} + \\ldots + \\phi_R\\rho_0\n\\end{align}\n\\] In the case of an AR(3)\n\\[\n\\begin{align}\n\\rho_1 & = \\phi_1 + \\phi_2 \\rho_1 + \\phi_3 \\rho_2\\\\\n\\rho_{2} & = \\phi_{1}\\rho_{2 - 1} + \\phi_2 \\rho_{2 - 2} + \\phi_3\\rho_{3 - 2}\\\\\n\\end{align}\n\\] Keep in mind that\n\\[\n\\rho(k) = \\rho(-k)\n\\]\n\\[\n\\begin{align}\n\\rho_1 & = \\phi_1 + \\phi_2 \\rho_1 + \\phi_3 \\rho_2\\\\\n\\rho_{2} & = \\phi_{1}\\rho_{1} + \\phi_2 \\rho_{0} + \\phi_3\\rho_{1}\\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\rho_1 = 0.5 + 0.34 \\rho_1 - 0.08 \\rho_2\\\\\n\\rho_2 = 0.5\\rho_1 + 0.34 - 0.08\\rho_1\\\\\n\\end{align}\n\\] This is a system of two unknowns and two equations. Its solution is left as an exercise."
  },
  {
    "objectID": "05-Model-Fitting.html#growth-series-1-quarter-to-previous-quarter",
    "href": "05-Model-Fitting.html#growth-series-1-quarter-to-previous-quarter",
    "title": "5  ARIMA Modeling (1)",
    "section": "5.1 Growth series 1 (quarter to previous quarter)",
    "text": "5.1 Growth series 1 (quarter to previous quarter)\nFor a series \\(y_1, \\ldots, y_T\\) the growth rate is given by\n\\[\ng_t = \\frac{y_t - y_{t - 1}}{y_{t - 1}} \\approx \\log(y_t) - \\log(y_{t - 1})\n\\]\n\ngdp &lt;- gdp %&gt;%\n  mutate(\n    growth1 = log(values) - log(lag(values))\n  )\n\n\ngdp %&gt;%\n  autoplot(growth1)\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\n\ngdp %&gt;%\n  gg_season(growth1)\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThis series still shows strong seasonality. We would like to avoid that for now, as our familiar arima models cannot handle seasonal effects (we will discuss this in the following weeks). Let us define the growth rate in a different way by comparing the GDP of a quarter with the GDP of the same quarter during the previous year.\n\nacf(gdp$growth1, na.action = na.pass)\n\n\n\npacf(gdp$growth1, na.action = na.pass)\n\n\n\n\n\nfit_AR1 &lt;- arima(gdp$growth1, order = c(1, 0, 0))\nfit_AR1\n\n\nCall:\narima(x = gdp$growth1, order = c(1, 0, 0))\n\nCoefficients:\n         ar1  intercept\n      -0.153     0.0087\ns.e.   0.093     0.0139\n\nsigma^2 estimated as 0.02925:  log likelihood = 39.55,  aic = -73.11\n\n\n\npredict(fit_AR1, n.ahead = 1)\n\n$pred\nTime Series:\nStart = 116 \nEnd = 116 \nFrequency = 1 \n[1] -0.01649729\n\n$se\nTime Series:\nStart = 116 \nEnd = 116 \nFrequency = 1 \n[1] 0.1710146\n\n\n\ntsdiag(fit_AR1)"
  },
  {
    "objectID": "06-Unit-Root-Tests.html",
    "href": "06-Unit-Root-Tests.html",
    "title": "6  DF-test",
    "section": "",
    "text": "7 Unit Root Tests\n# install.packages(\"urca\")\n\nlibrary(urca)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRandom walk model without a drift\n\\[\n\\begin{align}\ny_{t} = y_{t - 1} + e_{t}, e_t \\sim WN(\\sigma^2)\\\\\n\\Delta y_{t} = y_{t} - y_{t - 1} = e_{t}\\\\\nf' = \\lim_{\\epsilon \\to 0}\\frac{f(x) - f(x + \\epsilon)}{\\epsilon}\n\\end{align}\n\\] The solution of the characteristic equation of this model is equal to one. We say that the equation has a unit root.\n\\[\n\\lambda - 1 = 0 \\implies \\\\\n\\lambda^* = 1\n\\]\nThe solution of this characteristic equation is 1. We say that the characteristic equation has a unit root.\nARIMA(0, 1, 0)\nAR(1) process without a constant\n\\[\ny_{t} = \\phi_1 y_{t - 1} + e_{t}, e_t \\sim WN(\\sigma^2)\n\\] If the process has a unit root, it follows that \\(\\phi_1 = 1\\).\nWe would like to test the hypothesis:\n\\[\nH_0: \\phi_1 = 1\\\\\nH_1: \\phi_1 &lt; 1\n\\]\nThe first difference of the random walk process is stationary, because it is simply equal to the pure random process \\(e_{t}\\) and it is stationary by the properties of the pure random process. We say that the process (in this case the random walk process) is integrated of order 1."
  },
  {
    "objectID": "06-Unit-Root-Tests.html#random-walks-simulation",
    "href": "06-Unit-Root-Tests.html#random-walks-simulation",
    "title": "6  DF-test",
    "section": "7.1 Random Walks Simulation",
    "text": "7.1 Random Walks Simulation\n\nn &lt;- 200\nrw1 &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = n - 1)\nrw2 &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = n - 1)\n\n# rw2 &lt;- 1.5 * rw1 + rnorm(n = 200)\n\ntibble(\n  y = c(rw1, rw2),\n  process = rep(c(\"A\", \"B\"), each = 200),\n  time = rep(1:n, 2)\n) %&gt;%\n  ggplot(\n    aes(x = time, y = y, color = process)\n  ) + \n  geom_line()\n\n\n\n\n\nlinreg_fit &lt;- lm(rw2 ~ rw1)\nsummary(linreg_fit)\n\n\nCall:\nlm(formula = rw2 ~ rw1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5237 -3.7750 -0.7879  3.5170  9.1173 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.23541    0.73705   7.103 2.14e-11 ***\nrw1         -0.76211    0.06672 -11.423  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.464 on 198 degrees of freedom\nMultiple R-squared:  0.3972,    Adjusted R-squared:  0.3942 \nF-statistic: 130.5 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nRegression model\n\\[\ny_{t}: \\text{ random walk 2} \\\\\nx_{t}: \\text{ random walk 1} \\\\\ny_{t} = \\beta_0 + \\beta_1 x_{t} + u_{t}, u_{t} \\sim N(0, \\sigma^2)\n\\]\nEstimated regression equation\n\\[\n\\hat{y}_{t} = 6.15 - 1.78 x_{t}\n\\] T-Test of the hypothesis\n\\[\nH_0: \\beta_1 = 0\\\\\nH_1: \\beta_1 \\neq 0\n\\] We reject the null hypothesis if the p-value of the test is less than 0.05 (convention).\nSpurious regression if y and x are unit-root processes.\n\\[\n\\hat{y}_{t} = -1.86 + 0.19 x_{t}\n\\]"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#how-to-check-for-spurious-regression",
    "href": "06-Unit-Root-Tests.html#how-to-check-for-spurious-regression",
    "title": "6  DF-test",
    "section": "7.2 How to check for spurious regression?",
    "text": "7.2 How to check for spurious regression?\n\nres &lt;- residuals(linreg_fit)\nacf(res)"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#dickey-fuller-unit-root-test",
    "href": "06-Unit-Root-Tests.html#dickey-fuller-unit-root-test",
    "title": "6  DF-test",
    "section": "8.1 Dickey-Fuller Unit Root Test",
    "text": "8.1 Dickey-Fuller Unit Root Test\n\\[\n\\begin{align}\ny_{t} - y_{t - 1} = \\phi_1 y_{t - 1} + e_{t} - y_{t - 1} \\\\\n\\iff\\\\\n\\Delta y_{t} = (\\phi_1 - 1)y_{t - 1} + e_t\n\\end{align}\n\\]\n\\[\n\\phi_1 - 1 = 0 \\iff \\phi_1 = 1\n\\]\nWe can use the t-statistic for testing the hypothesis that the coefficient of \\(y_{t - 1}\\) equals 0, but the distribution of this statistic under the \\(H_0\\) (assuming that \\(\\phi_1 = 1\\)) is not a t-distribution. Therefore, we need to use special critical values in order to perform the test.\n\nsummary(ur.df(rw1, type = \"none\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression none \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9712 -0.6607 -0.1082  0.5918  2.7626 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)\nz.lag.1 0.001612   0.006559   0.246    0.806\n\nResidual standard error: 1.019 on 198 degrees of freedom\nMultiple R-squared:  0.0003051, Adjusted R-squared:  -0.004744 \nF-statistic: 0.06043 on 1 and 198 DF,  p-value: 0.8061\n\n\nValue of test-statistic is: 0.2458 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62\n\n\n-1 in the regression formula instructs lm to exclude the intercept from the regression equation.\n\\[\n\\underbrace{\\Delta y_{t}}_{z.diff} = \\underbrace{y_{t - 1}}_{z.lag.1} + e_t\n\\] t-value is the value of the t-statistic.\n\\[\nt = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\n-0.007011 / 0.008509 = -0.824\n\\] To perform the test, compare the t-value with the critical values:\n  1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62\nand we reject the null hypothesis if the t-value value is lower than the critical values.\n-0.824 &gt; -1.62 (the critical value corresponding to a 10% significance level), so we cannot reject the null hypothesis.\n\\[\nH_0: \\phi_1 - 1 = 0\n\\]"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#simulation-2-stationary-ar1-with-zero-level",
    "href": "06-Unit-Root-Tests.html#simulation-2-stationary-ar1-with-zero-level",
    "title": "6  DF-test",
    "section": "8.2 Simulation 2: Stationary AR(1) with zero level",
    "text": "8.2 Simulation 2: Stationary AR(1) with zero level\n\\[\ny_{t} = 0.7y_{t - 1} + e_{t}, e_t \\sim WN(1)\n\\]\n\nset.seed(5445)\nar1 &lt;- arima.sim(n = 200, model = list(ar = 0.7))\nplot(ar1)\n\n\n\n\n\nsummary(ur.df(ar1, type = \"none\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression none \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1957 -0.4637  0.1847  0.7274  2.8661 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nz.lag.1 -0.35559    0.05437   -6.54 5.12e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.981 on 198 degrees of freedom\nMultiple R-squared:  0.1776,    Adjusted R-squared:  0.1735 \nF-statistic: 42.77 on 1 and 198 DF,  p-value: 5.122e-10\n\n\nValue of test-statistic is: -6.5401 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62\n\n\n-6.54 &lt; -2.58 &lt; -1.95 =&gt; reject the null hypothesis of presence of a unit root"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#simulation-2-stationary-ar1-with-non-zero-level",
    "href": "06-Unit-Root-Tests.html#simulation-2-stationary-ar1-with-non-zero-level",
    "title": "6  DF-test",
    "section": "8.3 Simulation 2: Stationary AR(1) with non-zero level",
    "text": "8.3 Simulation 2: Stationary AR(1) with non-zero level\n\\[\ny_{t} = 100 + 0.7y_{t - 1} + e_{t}, e_t \\sim N(0, 1)\n\\]\n\nar2 &lt;- 100 + ar1\nplot(ar2)\n\n\n\n\n\nsummary(ur.df(ar2, type = \"none\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression none \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3897 -0.5694 -0.0054  0.6735  2.5924 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)\nz.lag.1 -4.129e-05  7.640e-04  -0.054    0.957\n\nResidual standard error: 1.082 on 198 degrees of freedom\nMultiple R-squared:  1.475e-05, Adjusted R-squared:  -0.005036 \nF-statistic: 0.00292 on 1 and 198 DF,  p-value: 0.957\n\n\nValue of test-statistic is: -0.054 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62\n\n\n\nsummary(ur.df(ar2, type = \"drift\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression drift \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2327 -0.6111  0.0591  0.6185  2.7507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 38.72772    5.64276   6.863 8.54e-11 ***\nz.lag.1     -0.38589    0.05622  -6.863 8.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9742 on 197 degrees of freedom\nMultiple R-squared:  0.193, Adjusted R-squared:  0.1889 \nF-statistic: 47.11 on 1 and 197 DF,  p-value: 8.529e-11\n\n\nValue of test-statistic is: -6.8635 23.554 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau2 -3.46 -2.88 -2.57\nphi1  6.52  4.63  3.81\n\n\nCompare the t-value (-6.253) to the critical values:\n  1pct  5pct 10pct\ntau2 -3.46 -2.88 -2.57\n-6.253 &lt; -3.46 (1-percent critical value) =&gt; we reject the null hypothesis at a 1-percent significance level."
  },
  {
    "objectID": "06-Unit-Root-Tests.html#simulation-3-stationary-ar1-with-trend",
    "href": "06-Unit-Root-Tests.html#simulation-3-stationary-ar1-with-trend",
    "title": "6  DF-test",
    "section": "8.4 Simulation 3 (Stationary AR(1) with trend)",
    "text": "8.4 Simulation 3 (Stationary AR(1) with trend)\n\\[\ny_{t} = 10 + 0.1t + 0.7 y_{t - 1} + e_{t}, e_t \\sim N(0, 1)\n\\]\n\nar3 &lt;- 10 + 0.1*(1:length(ar1)) + ar1\nplot(ar3)\n\n\n\n\n\nsummary(ur.df(ar3, type = \"drift\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression drift \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4678 -0.5735  0.0574  0.6901  2.4643 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.48017    0.28721   1.672   0.0961 .\nz.lag.1     -0.01859    0.01360  -1.367   0.1731  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 197 degrees of freedom\nMultiple R-squared:  0.0094,    Adjusted R-squared:  0.004372 \nF-statistic: 1.869 on 1 and 197 DF,  p-value: 0.1731\n\n\nValue of test-statistic is: -1.3673 1.8173 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau2 -3.46 -2.88 -2.57\nphi1  6.52  4.63  3.81\n\n\n-0.9984 &gt; -2.57 =&gt; the test fails to reject the null hypothesis at any reasonable significance level.\n\nsummary(ur.df(ar3, type = \"trend\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3040 -0.6054  0.0831  0.5956  2.6345 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.446414   0.632991   7.024 3.46e-11 ***\nz.lag.1     -0.402536   0.057255  -7.031 3.34e-11 ***\ntt           0.038493   0.005608   6.865 8.57e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9716 on 196 degrees of freedom\nMultiple R-squared:  0.2014,    Adjusted R-squared:  0.1933 \nF-statistic: 24.71 on 2 and 196 DF,  p-value: 2.681e-10\n\n\nValue of test-statistic is: -7.0306 17.2027 24.7148 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n\n\n  1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\n-6.2976 &lt; -3.99 =&gt; reject the null hypothesis at a 1-percent significance level."
  },
  {
    "objectID": "06-Unit-Root-Tests.html#adf-test",
    "href": "06-Unit-Root-Tests.html#adf-test",
    "title": "6  DF-test",
    "section": "8.5 ADF-Test",
    "text": "8.5 ADF-Test\nLet us consider an AR(2) process\n\\[\ny_{t} = \\phi_1 y_{t - 1} + \\phi_2 y_{t - 2} + e_t\n\\]\nIf this process has a unit root, then its characteristic equation\n\\[\n\\lambda ^2 - \\phi_1 \\lambda - \\phi_2 = 0\n\\]\nhas a solution \\(\\lambda^{*} = 1\\), therefore, setting \\(\\lambda = 1\\) satisfies the equation, and we obtain:\n\\[\n1^2 - \\phi_1 - \\phi_2 = 0\n\\]\nTherefore, if the process has a unit root, its coefficients sum to one. This is a hypothesis that we can test using data. Following the approach of the DF-Test, we can try to construct a regression equation where one of the coefficients is \\(1 - \\phi_1 - \\phi_2\\). Then we can conduct a test of the null hypothesis that this coefficient is zero.\nIt turns out that we can reparemetrize the AR(2) model to obtain such an equation:\n\\[\n\\begin{align}\ny_{t} - y_{t - 1} & = \\phi_1 y_{t - 1} - y_{t - 1} + \\phi_2 y_{t - 2} + e_t\\\\\n\\Delta y_{t} & = (\\phi_1 - 1)y_{t - 1} + \\phi_2 y_{t - 2} + e_t\\\\\n\\Delta y_{t} & = (\\phi_1 - 1)y_{t - 1} + \\phi_2 y_{t - 1} - \\phi_2 y_{t - 1} + \\phi_2 y_{t - 2} + e_t\\\\\n\\Delta y_{t} & = (\\phi_1 + \\phi_2 - 1)y_{t - 1} - \\phi_2 y_{t - 1} \\phi_2 y_{t - 2} + e_t\\\\\n\\Delta y_{t} & = (\\phi_1 + \\phi_2 - 1)y_{t - 1} - \\phi_2(y_{t - 1} - y_{t - 2}) + e_t\\\\\n\\Delta y_{t} & = (\\phi_1 + \\phi_2 - 1)y_{t - 1} - \\phi_2\\Delta y_{t - 1} + e_t\\\\\n\\end{align}\n\\]\nIn the general case of an AR(p) process the equation looks more complicated but it is derived in exactly the same way as for the AR(2) process:\n\\[\n\\Delta y_t = (\\rho - 1) y_{t - 1} + \\theta_{1} \\Delta y_{t - 1} + \\theta_2 \\Delta y_{t - 2} + \\ldots + \\theta_{p - 1} \\Delta y_{t - p + 1} + e_t\n\\]\nwhere \\(\\rho\\) and \\(\\theta_i\\) are defined as:\n\\[\n\\rho = \\sum_{j = 1}^p \\alpha_j \\\\\n\\theta_i = - \\sum_{j = i + 1}^{p} \\alpha_j\n\\] ## Information Criteria\n\\[\nAIC = -2 \\log(L) + 2(p + q + k + 1)\n\\]\n\\[\nAICe = -2 \\log(L) + 2(p + q + k + 1) + \\frac{2(p + q + k + 1)(p + q + k + 2)}{T - p - q - k - 2}\n\\]"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#simulation-ar3",
    "href": "06-Unit-Root-Tests.html#simulation-ar3",
    "title": "6  DF-test",
    "section": "8.6 Simulation: AR(3)",
    "text": "8.6 Simulation: AR(3)\n\\[\ny_t = 0.6y_{t - 1} + 0.4y_{t - 2} - 0.7y_{t - 2} + e_t, e_t \\sim N(0, 1)\n\\]\n\nset.seed(521)\nar4 &lt;- arima.sim(n = 200, model = list(ar = c(0.6, 0.4, -0.7)))\nplot(ar4)\n\n\n\n\n\nsummary(ur.df(ar4, type = \"none\", lags = 0))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression none \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2994 -0.9389 -0.1619  0.8093  4.0805 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nz.lag.1 -0.38591    0.05607  -6.883 7.54e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.302 on 198 degrees of freedom\nMultiple R-squared:  0.1931,    Adjusted R-squared:  0.189 \nF-statistic: 47.38 on 1 and 198 DF,  p-value: 7.543e-11\n\n\nValue of test-statistic is: -6.8832 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62\n\n\n\nsummary(ur.df(ar4, type = \"none\", selectlags = \"AIC\"))\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression none \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2490 -1.0398 -0.1433  0.7901  4.0562 \n\nCoefficients:\n           Estimate Std. Error t value Pr(&gt;|t|)    \nz.lag.1    -0.44249    0.06223  -7.110 2.11e-11 ***\nz.diff.lag  0.14540    0.07070   2.057   0.0411 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.294 on 196 degrees of freedom\nMultiple R-squared:  0.2096,    Adjusted R-squared:  0.2016 \nF-statistic: 25.99 on 2 and 196 DF,  p-value: 9.725e-11\n\n\nValue of test-statistic is: -7.1101 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau1 -2.58 -1.95 -1.62"
  },
  {
    "objectID": "06-Unit-Root-Tests.html#kpss-test",
    "href": "06-Unit-Root-Tests.html#kpss-test",
    "title": "6  DF-test",
    "section": "8.7 KPSS Test",
    "text": "8.7 KPSS Test\nThe KPSS test considers stationarity as the null hypothesis. Let’s consider the following process:\n\\[\ny_t = \\alpha_t + \\beta t + e_t, e_t \\sim WN(\\sigma^2)\n\\]\nwhere \\(\\alpha_t\\) is a random walk process: \\(\\alpha_t = \\alpha_{t - 1} + \\epsilon_t\\). If \\(y_t\\) is trend-stationary (i.e. has no unit roots), then the variance of \\(\\epsilon_t\\) is zero (i.e. \\(\\alpha_t\\) is a constant process).\nWith the KPSS test we reject the null hypothesis (trend-stationarity) for large values of the test statistic.\n\nsummary(ur.kpss(rw1))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 4 lags. \n\nValue of test-statistic is: 2.3152 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\nsummary(ur.kpss(ar1))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 4 lags. \n\nValue of test-statistic is: 0.398 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory\nTime Series with R. Use R!\nDordrecht Heidelberg: Springer. https://doi.org/10.1007/978-0-387-88698-5.\n\n\nKirchgässner, Gebhard, Jürgen Wolters, and Uwe Hassler. 2013.\nIntroduction to Modern Time Series Analysis. 2. ed. Springer\nTexts in Business and Economics. Berlin Heidelberg:\nSpringer.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. First edition.\nSebastopol, CA: O’Reilly."
  }
]