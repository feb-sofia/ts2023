
```{r}
library(tidyverse)
```

# Autoregressive Processes

Time series process: sequence of random variables

$$
y_1, y_2,\ldots,y_{T}
$$

For the rest of the course we will drop the distinction between the random variables (the stochastic process) and the observed values of the time series. You need to discern from the context whether we are talking about the random variables of about the values.


## The Purely Random Process

In the last class we discussed linear difference equations and their stability. Here we will introduce randomness into the difference equation by assuming that the autonomous terms $e_t$ are uncorrelated and follow a distribution with zero mean and constant variance (not depending on the time index).

:::{#def-purely-random}
## The Purely Random Process

A stochastic process $e_t$ with the follwing three properties is called a purely random or a white noise process.

$$
\begin{align}
& E(e_t) = 0 \text{ for all } t \\
& Var(e_t) = \sigma^2, \sigma^2 \in \mathbb{R} \\
& Cov(e_{t}, e_{t + k}) = 0, k \neq 0
\end{align}
$$

:::

From the definitions of variance (@def-variance) and covariance (@def-covariance) you can see that the
covariance between two time points is simply the expected value of the cross product of the terms, because $E(e_t) = 0$ for all $t$.

$$
Cov(e_t, e_{t + k}) = E(e_{t}e_{t + k}) = 0
$$

When $k = 0$, this expression reduces to

$$
Cov(e_t, e_{t + 0}) = E(e_t e_t) = E(e_t^2) = \sigma^2
$$
How does such a process look like? It is convenient to generate random values in order to visualize it.

```{r purely-random-sim}
n <- 100

dt <- tibble(
  t = 1:n,
  e = rnorm(
    # The number of values to be drawn at random
    n = n, 
    # The expected value of the distribution
    mean = 0,
    # The standard deviation of the distribution
    sd = 2
    )
)

dt %>%
  ggplot(aes(x = t, y = e)) + 
  geom_line()
```

In order to see how the two parameters of the normal distribution affect the 
shape of the resulting series, the following code chunk shows four series:

$$
\begin{align}
e^{(1)}_t & \sim N(\mu = 0, \sigma^2 = 1) \\
e^{(2)}_t & \sim N(\mu = 10, \sigma^2 = 1) \\
e^{(3)}_t & \sim N(\mu = 0, \sigma^2 = 3^2) \\
e^{(4)}_t & \sim N(\mu = 10, \sigma^2 = 3^2)
\end{align}
$$

```{r purely-random-mean-var}
## For illustration only

n <- 100

sim_data <- expand_grid(
  mu = c(0, 10),
  sigma = c(1, 3),
  t = 1:n
) %>%
  mutate(
    y = rnorm(n(), mean = mu, sd = sigma),
    sigma_lab = paste0("sigma = ", sigma),
    mu_lab = paste0("mu = ", mu)
  )

sim_data %>%
  ggplot(aes(x = t, y = y, color = mu_lab)) +
  geom_point(size = 1 / 2) +
  geom_line() +
  facet_wrap(~sigma_lab) +
  labs(
    x = "t",
    y = expression(y[t]),
    color = "Expected value"
  )
```

```{r}
sim_data %>%
  group_by(mu_lab, sigma_lab) %>%
  summarise(
    Average = mean(y),
    StdDev = sd(y)
  )
```

Now that we have seen some realizations of a couple of purely random processes, 
let's look at the empirical summaries of the time series. 

```{r}
mean(dt$e)
```

```{r}
sd(dt$e)
```

Let's also compute is (empirical) auto-correlation coefficients applying the
formula

$$
\hat{\rho}(k) = \frac{\hat{\gamma}(k)}{\hat{\gamma}(0)} \\
\hat{\rho}(k) = \frac{\sum_{t = 1}^{T - k}(y_t - \hat{\mu})(y_{t + k} - \hat{\mu})}{\sum_{t = 1}^{T}(y_t - \hat{\mu})}
$$

where $\hat{\mu}$ is the sample average of the series.

```{r}
dt <- dt %>%
  mutate(
    e_l1 = lag(e, n = 1),
    e_l2 = lag(e, n = 2),
    e_l3 = lag(e, n = 3)
  )
```

```{r}
cor(dt$e, dt$e_l1, use = "complete.obs")
cor(dt$e, dt$e_l2, use = "complete.obs")
cor(dt$e, dt$e_l3, use = "complete.obs")
```

You can obtain these auto-correlation coefficients using the `acf` function:

```{r}
acf(dt$e, plot = FALSE)
```

and you can visualize the auto-correlation coefficients

```{r}
acf(dt$e)
```

Even though the time series was generated from a process without zero correlations, 
the empirical auto-correlation coefficients will generally be non-zero. In order
to assess whether the observed correlations are compatible with zero theoretical
correlations, you can use the confidence interval 

$$
0 \pm \frac{2}{\sqrt{T}}
$$
that is shown as the two dashed horizontal lines in the plot.


The Box-Ljung test is a statistical test for the hypothesis

$$
\rho_{1} = \rho_2 = \rho_{k} = 0
$$

and is based on the test statistic:

$$
Q = T(T + 2)\sum_{j = 1}^{k}\frac{\hat{\rho}^2(j)}{T - j}
$$
Under the null hypothesis (i.e., if we assume the null hypothesis to be true)
it follows a $\Chi^2$ distribution with $k$ degrees of freedom.

```{r}
Box.test(dt$e, lag = 3)
```


## AR(1)

The autoregressive (AR) process of first order is defined by

$$
y_{t} = \phi_0 + \phi_1 y_{t - 1} + e_t
$$

where $\phi_0, \phi_1 \in \mathbb{R}$ are fixed (non-random) constants, and
$e_t$ is a purely random process.

We would like to derive the statistical properties of the process (expected value, variance and correlations) from the model definitions.


Let's give it a try. To derive it we use the model definition, the properties of the expected value operator from @thm-exp-value-props, and the properties of the purely random process in @def-purely-random. The expected value of the process is then

$$
\begin{align}
E(y_t) & = E(\phi_0 + \phi_1 y_{t - 1} + e_t) \\
       & = E(\phi_0) + E(\phi_1 y_{t - 1}) + E(e_t) \\
       & = \phi_0 + \phi_1 E(y_{t - 1}) + 0 \\
       & = \phi_0 + \phi_1 E(y_{t - 1})
\end{align}
$$
Now we may seem to have hit a dead end, because knowing the expected value of $E(y_t)$ requires the knowledge of $E(y_{t - 1})$. However, if we assume that the expected value does not depend on the time index, so that

$$
E(y_t) = E(y_{t - 1}) = \mu
$$

then the equation is very easy to solve:

$$
\underbrace{E(y_{t})}_{\mu} = \phi_0 + \phi_1 \underbrace{E(y_{t - 1})}_{\mu} \\
\mu = \phi_0 + \phi_1 \mu \\
(1 - \phi_1)\mu = \phi_0 \\
\mu = \frac{\phi_0}{1 - \phi_1}
$$

In other words 

$$
\mu = E(y_{t}) = E(y_{t - 1}) = \frac{\phi_0}{1 - \phi_1}
$$

This derivation begs the question: are we allowed to assume that the expected value does not change over time? The answer is yes, if the process is mean-stationary. This is just another way to say that the expected value does not change but we can derive a condition when this is the case.

To see how the stability condition for the first order difference equation relates
to the expected value, consider the solution to the equation that we derived last time. The only new thing here is the constant term $\phi_0$.

$$
y_{t} = \phi_1^{t} y_{0} + \frac{1 - \phi_1^{t}}{1 - \phi_1} \phi_0 + \sum_{k = 0}^{t - 1} \phi_1^{k} u_{t - k}
$$



:::{#exr-variance-ar-1}
## Variance of a Stationary AR(1)

What is the variance of an AR(1) process given by:

$$
y_{t} = \phi_0 + \phi_1 y_{t - 1} + e_{t}, e_{t} \sim N(0, \sigma^2)
$$
Use the properties of the variance in @thm-variance-short and the properties of the purely random process to derive it.

$$
Var(y_{t}) = ?
$$

:::

:::{#exr-ar-1-cor}
## Autocorrelations of a AR(1) process

Express the first and second order auto-correlations of the AR(1) process in terms of its coefficients $\phi_0$ and $\phi_1$. Without a loss of generality, assume that the process has a zero expected value.

:::

## Exercise

## AR(3)

$$
y_{t} = \delta + \phi_{1}y_{t - 1} + \phi_2 y_{t - 2} + \phi_3 y_{t - 3} + e_t, \quad e_t \sim WN(\sigma^2)
$$
Roots of the lag-polynomial: 0.8, 0.2, -0.5. Roots of what?

$$
Ly_{t} = y_{t - 1}\\
Ly_{t - 1} = y_{t - 2}\\
L\cdot Ly_{t} = y_{t - 2}\\
L^2y_{t} = y_{t - 2}\\
L^3y_{t} = y_{t - 3}
$$

Using the lag operator we can express the model as:
$$
y_{t} = \delta + \phi_1 Ly_{t} + \phi_2 L^2y_{t} + \phi_3 L^3 y_{t} + e_t\\
y_{t}(\underbrace{L^0 - \phi_1 L - \phi_2 L^2 - \phi_3 L^3}_{\text{lag polynomial}}) = \delta + e_t
$$
Characteristic equation:

$$
\lambda^{3 - 0} - \phi_1\lambda^{3 - 1} - \phi_2 \lambda^{3 - 2} - \phi_3 \lambda ^ {3 - 3} = 0\\
\lambda^{3} - \phi_1\lambda^{2} - \phi_2 \lambda - \phi_3 = 0
$$
The roots of the characteristic polynomial determine whether the process is stationary
or not. If the roots of this equation are less than 1 in absolute value then the process
is stationary.

If we know that the roots of this equation are

$$
\lambda_1^* = 0.8, \lambda_2^* = 0.2, \lambda_3^* = -0.5
$$
$$
\lambda^{3} - \phi_1\lambda^{2} - \phi_2 \lambda - \phi_3 = (\lambda - \lambda_{1}^*)(\lambda - \lambda_2^*)(\lambda - \lambda_3^*)
$$
$$
\begin{align}
(\lambda - \lambda_{1}^*)(\lambda - \lambda_2^*)(\lambda - \lambda_3^*) = \\
(\lambda - 0.8)(\lambda - 0.2)(\lambda + 0.5) = \\
\left[\lambda^2 - \lambda + 0.16\right](\lambda + 0.5) = \\
\lambda^3 - \lambda^2 + 0.16\lambda + 0.5\lambda^2 - 0.5\lambda + 0.08
\end{align}
$$

$$
\lambda^{3} - \phi_1\lambda^{2} - \phi_2 \lambda - \phi_3\\
\lambda^3 -0.5\lambda^2 -0.34\lambda + 0.08
$$

$$
\phi_1 = 0.5\\
\phi_2 = 0.34\\
\phi_3 = -0.08
$$
$$
\begin{align}
y_{t} & = \phi_0 + \phi_{1}y_{t - 1} + \phi_2 y_{t - 2} + \phi_3 y_{t - 3} + e_t\\
y_{t} & = \phi_0 + 0.5y_{t - 1} + 0.34y_{t - 2} - 0.08 y_{t - 3} + e_t
\end{align}
$$
Now we can calculate

First order autocorrelation:
$$
\rho(y_{t}, y_{t - 1}) = \rho_1 = \frac{Cov(y_{t}, y_{t - 1})}{Var(y_{t})}
$$
The Yule-Walker equations connect the autocorrelations (and autocovariances) and
the model coefficients.

$$
\begin{align}
\rho_1 & = \phi_1 \rho_0 + \phi_2\rho_1 + \ldots + \phi_R\rho_{R - 1}\\
\rho_2  & = \phi_1 \rho_1 + \phi_2 + \ldots + \phi_R \rho_{R - 2} \\
\vdots\\
\rho_{R - 1}  & = \phi_1 \rho_{R - 1} + \phi_2\rho_{R - 2} + \ldots + \phi_R\rho_0
\end{align}
$$
In the case of an AR(3)

$$
\begin{align}
\rho_1 & = \phi_1 + \phi_2 \rho_1 + \phi_3 \rho_2\\
\rho_{2} & = \phi_{1}\rho_{2 - 1} + \phi_2 \rho_{2 - 2} + \phi_3\rho_{3 - 2}\\
\end{align}
$$
Keep in mind that

$$
\rho(k) = \rho(-k)
$$

$$
\begin{align}
\rho_1 & = \phi_1 + \phi_2 \rho_1 + \phi_3 \rho_2\\
\rho_{2} & = \phi_{1}\rho_{1} + \phi_2 \rho_{0} + \phi_3\rho_{1}\\
\end{align}
$$

$$
\begin{align}
\rho_1 = 0.5 + 0.34 \rho_1 - 0.08 \rho_2\\
\rho_2 = 0.5\rho_1 + 0.34 - 0.08\rho_1\\
\end{align}
$$
This is a system of two unknowns and two equations. Its solution is left as an exercise.

